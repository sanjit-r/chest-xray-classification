{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from torchvision.models import densenet121, DenseNet121_Weights\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xray_dataset import XRayDataset\n",
    "from transforms import equalize\n",
    "\n",
    "# Set up the device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Cardiomegaly', 'Emphysema', 'Effusion', 'No Finding', 'Hernia', 'Infiltration', 'Mass', 'Nodule', 'Atelectasis', 'Pneumothorax', 'Pleural_Thickening', 'Pneumonia', 'Fibrosis', 'Edema', 'Consolidation']\n",
      "\n",
      "bad_labels:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Inverted</th>\n",
       "      <th>Not frontal</th>\n",
       "      <th>Rotated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000583_024.png</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002180_000.png</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00002300_026.png</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00002371_015.png</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00006209_001.png</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Index  Inverted  Not frontal  Rotated\n",
       "0  00000583_024.png       1.0          0.0      0.0\n",
       "1  00002180_000.png       1.0          0.0      0.0\n",
       "2  00002300_026.png       1.0          0.0      0.0\n",
       "3  00002371_015.png       1.0          0.0      0.0\n",
       "4  00006209_001.png       1.0          0.0      0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_df:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Emphysema</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Hernia</th>\n",
       "      <th>Infiltration</th>\n",
       "      <th>Mass</th>\n",
       "      <th>Nodule</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural_Thickening</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Fibrosis</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>FilePath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000001_000.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>../input/data/images_001/images/00000001_000.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000001_001.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>../input/data/images_001/images/00000001_001.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000001_002.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>../input/data/images_001/images/00000001_002.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000002_000.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>../input/data/images_001/images/00000002_000.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000003_000.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>../input/data/images_001/images/00000003_000.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Image Index  Cardiomegaly  Emphysema  Effusion  No Finding  Hernia  \\\n",
       "0  00000001_000.png             1          0         0           0       0   \n",
       "1  00000001_001.png             1          1         0           0       0   \n",
       "2  00000001_002.png             1          0         1           0       0   \n",
       "3  00000002_000.png             0          0         0           1       0   \n",
       "4  00000003_000.png             0          0         0           0       1   \n",
       "\n",
       "   Infiltration  Mass  Nodule  Atelectasis  Pneumothorax  Pleural_Thickening  \\\n",
       "0             0     0       0            0             0                   0   \n",
       "1             0     0       0            0             0                   0   \n",
       "2             0     0       0            0             0                   0   \n",
       "3             0     0       0            0             0                   0   \n",
       "4             0     0       0            0             0                   0   \n",
       "\n",
       "   Pneumonia  Fibrosis  Edema  Consolidation  \\\n",
       "0          0         0      0              0   \n",
       "1          0         0      0              0   \n",
       "2          0         0      0              0   \n",
       "3          0         0      0              0   \n",
       "4          0         0      0              0   \n",
       "\n",
       "                                           FilePath  \n",
       "0  ../input/data/images_001/images/00000001_000.png  \n",
       "1  ../input/data/images_001/images/00000001_001.png  \n",
       "2  ../input/data/images_001/images/00000001_002.png  \n",
       "3  ../input/data/images_001/images/00000002_000.png  \n",
       "4  ../input/data/images_001/images/00000003_000.png  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m6/n94d18910t93n63044f2y9cm0000gn/T/ipykernel_13934/4196785590.py:28: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  img_dict[row[0]].append(np.array(row[1:-1], dtype=int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total image files found : 112120\n"
     ]
    }
   ],
   "source": [
    "# make sure data is in root directory\n",
    "data_dir = '/Users/ruchireddy/nih-chest-xray-dataset/'\n",
    "bad_labels = pd.read_csv(data_dir + 'cxr14_bad_labels.csv')\n",
    "bad_labels = bad_labels.loc[:, ~bad_labels.columns.str.contains('^Unnamed')]\n",
    "all_df = pd.read_csv(data_dir + 'train_df.csv')\n",
    "all_df = all_df.drop(columns=['Patient ID'])\n",
    "all_df = all_df.dropna()\n",
    "\n",
    "LABELS = all_df.columns.tolist()[1:-1]\n",
    "print(\"Labels:\", LABELS)\n",
    "print()\n",
    "\n",
    "print('bad_labels:')\n",
    "display(bad_labels.head())\n",
    "print()\n",
    "\n",
    "print('all_df:')\n",
    "display(all_df.head())\n",
    "\n",
    "img_dict = {}\n",
    "for root, _, files in os.walk(data_dir):\n",
    "    for file_name in files:\n",
    "        if file_name.endswith(\".png\"):\n",
    "            full_path = os.path.join(root, file_name)\n",
    "            img_dict[file_name] = [full_path]\n",
    "\n",
    "for index, row in all_df.iterrows():\n",
    "    img_dict[row[0]].append(np.array(row[1:-1], dtype=int))\n",
    "\n",
    "# List all files in the data directory\n",
    "print(f'Total image files found : {len(img_dict.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_dict length before dropping bad labels:  112120\n",
      "total number of bad_labels:  (432, 4)\n",
      "img_dict length after dropping bad labels:  111863\n"
     ]
    }
   ],
   "source": [
    "print('img_dict length before dropping bad labels: ', len(img_dict.keys()))\n",
    "print('total number of bad_labels: ', bad_labels.shape)\n",
    "\n",
    "# Remove images from img_dict that exist in bad_labels\n",
    "for bad_image in bad_labels['Index']:\n",
    "    if bad_image in img_dict:\n",
    "        del img_dict[bad_image]\n",
    "\n",
    "print('img_dict length after dropping bad labels: ', len(img_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/ruchireddy/nih-chest-xray-dataset/images_001/images/00000001_000.png',\n",
       " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_dict['00000001_000.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 80% train and 20% test\n",
    "train_img_paths_and_labels, test_img_paths_and_labels = train_test_split(list(img_dict.values()), test_size=0.3, random_state=42)\n",
    "\n",
    "# split into validation and test\n",
    "test_img_paths_and_labels, val_img_paths_and_labels = train_test_split(test_img_paths_and_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "#  Define a transformation to convert images to tensors\n",
    "train_transform = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    # transforms.RandomHorizontalFlip(),    # horizontal flip could not be consistent with the labels for xray images\n",
    "    # transforms.RandomRotation(10),    rotations not advised for x-ray images\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "    transforms.Lambda(equalize),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create a dataset class for the XRay images\n",
    "train_dataset = XRayDataset(train_img_paths_and_labels, transform=train_transform)\n",
    "val_dataset = XRayDataset(val_img_paths_and_labels, transform=test_transform)\n",
    "test_dataset = XRayDataset(test_img_paths_and_labels, transform=test_transform)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224]) 64\n"
     ]
    }
   ],
   "source": [
    "# check if the data is loaded correctly\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "print(images.shape, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print a sample image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax\n",
    "\n",
    "# Display the image\n",
    "imshow(images[0], normalize=True)\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = inputs.float()\n",
    "        targets = targets.float()\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        p_t = torch.where(targets == 1, probs, 1 - probs)\n",
    "        focal_factor = (1 - p_t) ** self.gamma\n",
    "        loss = self.alpha * focal_factor * BCE_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the model\n",
    "class XRayModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(XRayModel, self).__init__()\n",
    "        self.densenet = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
    "        num_feats = self.densenet.classifier.in_features\n",
    "        self.densenet.classifier = nn.Sequential(\n",
    "            nn.Linear(num_feats, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "       \n",
    "    def forward(self, x):\n",
    "        x = self.densenet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, optimizer, and loss function\n",
    "model = XRayModel(num_classes=len(LABELS))\n",
    "model.to(device)\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = FocalLoss()\n",
    "num_epochs = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [1/1224], Loss: 0.042823053896427155\n",
      "Batch [2/1224], Loss: 0.01900273747742176\n",
      "Batch [3/1224], Loss: 0.01956784911453724\n",
      "Batch [4/1224], Loss: 0.01598248817026615\n",
      "Batch [5/1224], Loss: 0.018522094935178757\n",
      "Batch [6/1224], Loss: 0.02133447863161564\n",
      "Batch [7/1224], Loss: 0.015782754868268967\n",
      "Batch [8/1224], Loss: 0.01672561652958393\n",
      "Batch [9/1224], Loss: 0.015340062789618969\n",
      "Batch [10/1224], Loss: 0.016412919387221336\n",
      "Batch [11/1224], Loss: 0.016173377633094788\n",
      "Batch [12/1224], Loss: 0.018537888303399086\n",
      "Batch [13/1224], Loss: 0.015148982405662537\n",
      "Batch [14/1224], Loss: 0.01635126583278179\n",
      "Batch [15/1224], Loss: 0.015832792967557907\n",
      "Batch [16/1224], Loss: 0.01570129580795765\n",
      "Batch [17/1224], Loss: 0.015568257309496403\n",
      "Batch [18/1224], Loss: 0.010142013430595398\n",
      "Batch [19/1224], Loss: 0.017368368804454803\n",
      "Batch [20/1224], Loss: 0.014346049167215824\n",
      "Batch [21/1224], Loss: 0.014766816049814224\n",
      "Batch [22/1224], Loss: 0.01599353551864624\n",
      "Batch [23/1224], Loss: 0.012190328910946846\n",
      "Batch [24/1224], Loss: 0.01460827887058258\n",
      "Batch [25/1224], Loss: 0.01690797135233879\n",
      "Batch [26/1224], Loss: 0.016654793173074722\n",
      "Batch [27/1224], Loss: 0.017503125593066216\n",
      "Batch [28/1224], Loss: 0.014555207453668118\n",
      "Batch [29/1224], Loss: 0.0147117730230093\n",
      "Batch [30/1224], Loss: 0.01596239022910595\n",
      "Batch [31/1224], Loss: 0.01856681890785694\n",
      "Batch [32/1224], Loss: 0.014195836149156094\n",
      "Batch [33/1224], Loss: 0.01650935225188732\n",
      "Batch [34/1224], Loss: 0.013287384063005447\n",
      "Batch [35/1224], Loss: 0.01475565880537033\n",
      "Batch [36/1224], Loss: 0.013986989855766296\n",
      "Batch [37/1224], Loss: 0.01402235310524702\n",
      "Batch [38/1224], Loss: 0.014582831412553787\n",
      "Batch [39/1224], Loss: 0.013602525927126408\n",
      "Batch [40/1224], Loss: 0.015318787656724453\n",
      "Batch [41/1224], Loss: 0.015959322452545166\n",
      "Batch [42/1224], Loss: 0.015068688429892063\n",
      "Batch [43/1224], Loss: 0.014652789570391178\n",
      "Batch [44/1224], Loss: 0.012907460331916809\n",
      "Batch [45/1224], Loss: 0.015903757885098457\n",
      "Batch [46/1224], Loss: 0.015580374747514725\n",
      "Batch [47/1224], Loss: 0.017676925286650658\n",
      "Batch [48/1224], Loss: 0.013469474390149117\n",
      "Batch [49/1224], Loss: 0.016609081998467445\n",
      "Batch [50/1224], Loss: 0.014013801701366901\n",
      "Batch [51/1224], Loss: 0.015222953632473946\n",
      "Batch [52/1224], Loss: 0.016078896820545197\n",
      "Batch [53/1224], Loss: 0.015122625976800919\n",
      "Batch [54/1224], Loss: 0.012811370193958282\n",
      "Batch [55/1224], Loss: 0.014842628501355648\n",
      "Batch [56/1224], Loss: 0.015101783908903599\n",
      "Batch [57/1224], Loss: 0.015018386766314507\n",
      "Batch [58/1224], Loss: 0.01341292355209589\n",
      "Batch [59/1224], Loss: 0.014543125405907631\n",
      "Batch [60/1224], Loss: 0.013080493547022343\n",
      "Batch [61/1224], Loss: 0.012132407166063786\n",
      "Batch [62/1224], Loss: 0.012975476682186127\n",
      "Batch [63/1224], Loss: 0.01199489738792181\n",
      "Batch [64/1224], Loss: 0.014888003468513489\n",
      "Batch [65/1224], Loss: 0.013341019861400127\n",
      "Batch [66/1224], Loss: 0.013771812431514263\n",
      "Batch [67/1224], Loss: 0.013084512203931808\n",
      "Batch [68/1224], Loss: 0.015657272189855576\n",
      "Batch [69/1224], Loss: 0.013673629611730576\n",
      "Batch [70/1224], Loss: 0.014846145175397396\n",
      "Batch [71/1224], Loss: 0.018443332985043526\n",
      "Batch [72/1224], Loss: 0.015743358060717583\n",
      "Batch [73/1224], Loss: 0.015070912428200245\n",
      "Batch [74/1224], Loss: 0.01575629971921444\n",
      "Batch [75/1224], Loss: 0.015177212655544281\n",
      "Batch [76/1224], Loss: 0.015926135703921318\n",
      "Batch [77/1224], Loss: 0.014862410724163055\n",
      "Batch [78/1224], Loss: 0.009928364306688309\n",
      "Batch [79/1224], Loss: 0.014810650609433651\n",
      "Batch [80/1224], Loss: 0.014080716297030449\n",
      "Batch [81/1224], Loss: 0.012578891590237617\n",
      "Batch [82/1224], Loss: 0.015885278582572937\n",
      "Batch [83/1224], Loss: 0.015585778281092644\n",
      "Batch [84/1224], Loss: 0.016691863536834717\n",
      "Batch [85/1224], Loss: 0.015077321790158749\n",
      "Batch [86/1224], Loss: 0.015318537130951881\n",
      "Batch [87/1224], Loss: 0.01603623479604721\n",
      "Batch [88/1224], Loss: 0.014877507463097572\n",
      "Batch [89/1224], Loss: 0.012331484816968441\n",
      "Batch [90/1224], Loss: 0.013209182769060135\n",
      "Batch [91/1224], Loss: 0.014128283597528934\n",
      "Batch [92/1224], Loss: 0.015123425982892513\n",
      "Batch [93/1224], Loss: 0.019187817350029945\n",
      "Batch [94/1224], Loss: 0.016772205010056496\n",
      "Batch [95/1224], Loss: 0.015196416527032852\n",
      "Batch [96/1224], Loss: 0.014722810126841068\n",
      "Batch [97/1224], Loss: 0.015429669059813023\n",
      "Batch [98/1224], Loss: 0.014384878799319267\n",
      "Batch [99/1224], Loss: 0.015368420630693436\n",
      "Batch [100/1224], Loss: 0.01690559647977352\n",
      "Batch [101/1224], Loss: 0.017288435250520706\n",
      "Batch [102/1224], Loss: 0.01365010254085064\n",
      "Batch [103/1224], Loss: 0.014086260460317135\n",
      "Batch [104/1224], Loss: 0.015014195814728737\n",
      "Batch [105/1224], Loss: 0.016552552580833435\n",
      "Batch [106/1224], Loss: 0.014181755483150482\n",
      "Batch [107/1224], Loss: 0.01191322784870863\n",
      "Batch [108/1224], Loss: 0.012260651215910912\n",
      "Batch [109/1224], Loss: 0.010959004051983356\n",
      "Batch [110/1224], Loss: 0.01239599660038948\n",
      "Batch [111/1224], Loss: 0.013148725964128971\n",
      "Batch [112/1224], Loss: 0.018560675904154778\n",
      "Batch [113/1224], Loss: 0.014697335660457611\n",
      "Batch [114/1224], Loss: 0.01915893517434597\n",
      "Batch [115/1224], Loss: 0.014596113935112953\n",
      "Batch [116/1224], Loss: 0.015898868441581726\n",
      "Batch [117/1224], Loss: 0.016651339828968048\n",
      "Batch [118/1224], Loss: 0.017637232318520546\n",
      "Batch [119/1224], Loss: 0.014028269797563553\n",
      "Batch [120/1224], Loss: 0.013574815355241299\n",
      "Batch [121/1224], Loss: 0.013705030083656311\n",
      "Batch [122/1224], Loss: 0.013589845038950443\n",
      "Batch [123/1224], Loss: 0.013927561230957508\n",
      "Batch [124/1224], Loss: 0.016072358936071396\n",
      "Batch [125/1224], Loss: 0.018887681886553764\n",
      "Batch [126/1224], Loss: 0.015240288339555264\n",
      "Batch [127/1224], Loss: 0.012889273464679718\n",
      "Batch [128/1224], Loss: 0.016575967893004417\n",
      "Batch [129/1224], Loss: 0.015639418736100197\n",
      "Batch [130/1224], Loss: 0.014363589696586132\n",
      "Batch [131/1224], Loss: 0.014450175687670708\n",
      "Batch [132/1224], Loss: 0.017785077914595604\n",
      "Batch [133/1224], Loss: 0.014259411953389645\n",
      "Batch [134/1224], Loss: 0.014754526317119598\n",
      "Batch [135/1224], Loss: 0.014735206961631775\n",
      "Batch [136/1224], Loss: 0.01582912728190422\n",
      "Batch [137/1224], Loss: 0.01657266356050968\n",
      "Batch [138/1224], Loss: 0.01305315736681223\n",
      "Batch [139/1224], Loss: 0.015580260194838047\n",
      "Batch [140/1224], Loss: 0.013485516421496868\n",
      "Batch [141/1224], Loss: 0.013113407418131828\n",
      "Batch [142/1224], Loss: 0.015098042786121368\n",
      "Batch [143/1224], Loss: 0.013170924037694931\n",
      "Batch [144/1224], Loss: 0.013586897403001785\n",
      "Batch [145/1224], Loss: 0.015546509064733982\n",
      "Batch [146/1224], Loss: 0.013341122306883335\n",
      "Batch [147/1224], Loss: 0.014923986047506332\n",
      "Batch [148/1224], Loss: 0.014461299404501915\n",
      "Batch [149/1224], Loss: 0.014276029542088509\n",
      "Batch [150/1224], Loss: 0.015531430020928383\n",
      "Batch [151/1224], Loss: 0.01649663969874382\n",
      "Batch [152/1224], Loss: 0.01410106010735035\n",
      "Batch [153/1224], Loss: 0.013928791508078575\n",
      "Batch [154/1224], Loss: 0.013756711967289448\n",
      "Batch [155/1224], Loss: 0.01555364578962326\n",
      "Batch [156/1224], Loss: 0.014065300114452839\n",
      "Batch [157/1224], Loss: 0.015787266194820404\n",
      "Batch [158/1224], Loss: 0.01386097352951765\n",
      "Batch [159/1224], Loss: 0.015273612923920155\n",
      "Batch [160/1224], Loss: 0.017806796357035637\n",
      "Batch [161/1224], Loss: 0.014434467069804668\n",
      "Batch [162/1224], Loss: 0.015663377940654755\n",
      "Batch [163/1224], Loss: 0.014824196696281433\n",
      "Batch [164/1224], Loss: 0.01295996829867363\n",
      "Batch [165/1224], Loss: 0.014981167390942574\n",
      "Batch [166/1224], Loss: 0.013864551670849323\n",
      "Batch [167/1224], Loss: 0.015158629044890404\n",
      "Batch [168/1224], Loss: 0.015735281631350517\n",
      "Batch [169/1224], Loss: 0.014482250437140465\n",
      "Batch [170/1224], Loss: 0.015234976075589657\n",
      "Batch [171/1224], Loss: 0.013711993582546711\n",
      "Batch [172/1224], Loss: 0.015312885865569115\n",
      "Batch [173/1224], Loss: 0.01260517630726099\n",
      "Batch [174/1224], Loss: 0.015882134437561035\n",
      "Batch [175/1224], Loss: 0.01471452135592699\n",
      "Batch [176/1224], Loss: 0.015996135771274567\n",
      "Batch [177/1224], Loss: 0.01511464174836874\n",
      "Batch [178/1224], Loss: 0.010908206924796104\n",
      "Batch [179/1224], Loss: 0.012275357730686665\n",
      "Batch [180/1224], Loss: 0.01773829571902752\n",
      "Batch [181/1224], Loss: 0.013958180323243141\n",
      "Batch [182/1224], Loss: 0.012684451416134834\n",
      "Batch [183/1224], Loss: 0.013611928559839725\n",
      "Batch [184/1224], Loss: 0.019735580310225487\n",
      "Batch [185/1224], Loss: 0.015360573306679726\n",
      "Batch [186/1224], Loss: 0.015438702888786793\n",
      "Batch [187/1224], Loss: 0.014621052891016006\n",
      "Batch [188/1224], Loss: 0.016054583713412285\n",
      "Batch [189/1224], Loss: 0.01587606780230999\n",
      "Batch [190/1224], Loss: 0.012934955768287182\n",
      "Batch [191/1224], Loss: 0.015751058235764503\n",
      "Batch [192/1224], Loss: 0.014023222960531712\n",
      "Batch [193/1224], Loss: 0.015056896954774857\n",
      "Batch [194/1224], Loss: 0.014027186669409275\n",
      "Batch [195/1224], Loss: 0.015997741371393204\n",
      "Batch [196/1224], Loss: 0.014858270063996315\n",
      "Batch [197/1224], Loss: 0.01587020233273506\n",
      "Batch [198/1224], Loss: 0.013604245148599148\n",
      "Batch [199/1224], Loss: 0.014852715656161308\n",
      "Batch [200/1224], Loss: 0.015229815617203712\n",
      "Batch [201/1224], Loss: 0.014887006022036076\n",
      "Batch [202/1224], Loss: 0.015001850202679634\n",
      "Batch [203/1224], Loss: 0.013141226023435593\n",
      "Batch [204/1224], Loss: 0.01274796761572361\n",
      "Batch [205/1224], Loss: 0.014079633168876171\n",
      "Batch [206/1224], Loss: 0.01610979065299034\n",
      "Batch [207/1224], Loss: 0.014500880613923073\n",
      "Batch [208/1224], Loss: 0.01545643713325262\n",
      "Batch [209/1224], Loss: 0.015272291377186775\n",
      "Batch [210/1224], Loss: 0.019324855878949165\n",
      "Batch [211/1224], Loss: 0.013872969895601273\n",
      "Batch [212/1224], Loss: 0.013449953868985176\n",
      "Batch [213/1224], Loss: 0.014771857298910618\n",
      "Batch [214/1224], Loss: 0.01425048429518938\n",
      "Batch [215/1224], Loss: 0.014828434213995934\n",
      "Batch [216/1224], Loss: 0.014947333373129368\n",
      "Batch [217/1224], Loss: 0.013785142451524734\n",
      "Batch [218/1224], Loss: 0.012127798981964588\n",
      "Batch [219/1224], Loss: 0.017440419644117355\n",
      "Batch [220/1224], Loss: 0.015536686405539513\n",
      "Batch [221/1224], Loss: 0.011892111040651798\n",
      "Batch [222/1224], Loss: 0.013855897821485996\n",
      "Batch [223/1224], Loss: 0.014990927651524544\n",
      "Batch [224/1224], Loss: 0.013101087883114815\n",
      "Batch [225/1224], Loss: 0.014145873486995697\n",
      "Batch [226/1224], Loss: 0.013697792775928974\n",
      "Batch [227/1224], Loss: 0.01594843901693821\n",
      "Batch [228/1224], Loss: 0.015456077642738819\n",
      "Batch [229/1224], Loss: 0.014318332076072693\n",
      "Batch [230/1224], Loss: 0.015826305374503136\n",
      "Batch [231/1224], Loss: 0.0151927899569273\n",
      "Batch [232/1224], Loss: 0.01523195207118988\n",
      "Batch [233/1224], Loss: 0.015748482197523117\n",
      "Batch [234/1224], Loss: 0.015229474753141403\n",
      "Batch [235/1224], Loss: 0.014055917970836163\n",
      "Batch [236/1224], Loss: 0.013177991844713688\n",
      "Batch [237/1224], Loss: 0.01398563850671053\n",
      "Batch [238/1224], Loss: 0.015651486814022064\n",
      "Batch [239/1224], Loss: 0.01362408883869648\n",
      "Batch [240/1224], Loss: 0.014567125588655472\n",
      "Batch [241/1224], Loss: 0.012504003010690212\n",
      "Batch [242/1224], Loss: 0.01641407050192356\n",
      "Batch [243/1224], Loss: 0.01172175258398056\n",
      "Batch [244/1224], Loss: 0.01292863767594099\n",
      "Batch [245/1224], Loss: 0.01578013226389885\n",
      "Batch [246/1224], Loss: 0.014559401199221611\n",
      "Batch [247/1224], Loss: 0.013464443385601044\n",
      "Batch [248/1224], Loss: 0.012729154899716377\n",
      "Batch [249/1224], Loss: 0.015319669619202614\n",
      "Batch [250/1224], Loss: 0.013967705890536308\n",
      "Batch [251/1224], Loss: 0.014840063638985157\n",
      "Batch [252/1224], Loss: 0.012400269508361816\n",
      "Batch [253/1224], Loss: 0.016749940812587738\n",
      "Batch [254/1224], Loss: 0.012899125926196575\n",
      "Batch [255/1224], Loss: 0.014017393812537193\n",
      "Batch [256/1224], Loss: 0.015437410213053226\n",
      "Batch [257/1224], Loss: 0.014443328604102135\n",
      "Batch [258/1224], Loss: 0.011911661364138126\n",
      "Batch [259/1224], Loss: 0.014038719236850739\n",
      "Batch [260/1224], Loss: 0.01634073071181774\n",
      "Batch [261/1224], Loss: 0.013019558042287827\n",
      "Batch [262/1224], Loss: 0.014642537571489811\n",
      "Batch [263/1224], Loss: 0.015601432882249355\n",
      "Batch [264/1224], Loss: 0.015892377123236656\n",
      "Batch [265/1224], Loss: 0.015176095999777317\n",
      "Batch [266/1224], Loss: 0.01438155584037304\n",
      "Batch [267/1224], Loss: 0.01388107892125845\n",
      "Batch [268/1224], Loss: 0.012450757436454296\n",
      "Batch [269/1224], Loss: 0.014696823433041573\n",
      "Batch [270/1224], Loss: 0.015921691432595253\n",
      "Batch [271/1224], Loss: 0.01820504106581211\n",
      "Batch [272/1224], Loss: 0.017328418791294098\n",
      "Batch [273/1224], Loss: 0.012827938422560692\n",
      "Batch [274/1224], Loss: 0.013825232163071632\n",
      "Batch [275/1224], Loss: 0.014948912896215916\n",
      "Batch [276/1224], Loss: 0.015105387195944786\n",
      "Batch [277/1224], Loss: 0.014655118808150291\n",
      "Batch [278/1224], Loss: 0.014828057028353214\n",
      "Batch [279/1224], Loss: 0.013035628013312817\n",
      "Batch [280/1224], Loss: 0.013919581659138203\n",
      "Batch [281/1224], Loss: 0.013936595991253853\n",
      "Batch [282/1224], Loss: 0.01712019182741642\n",
      "Batch [283/1224], Loss: 0.015064341016113758\n",
      "Batch [284/1224], Loss: 0.014333405531942844\n",
      "Batch [285/1224], Loss: 0.011408433318138123\n",
      "Batch [286/1224], Loss: 0.014794055372476578\n",
      "Batch [287/1224], Loss: 0.015428830869495869\n",
      "Batch [288/1224], Loss: 0.013410413637757301\n",
      "Batch [289/1224], Loss: 0.014252792112529278\n",
      "Batch [290/1224], Loss: 0.016024384647607803\n",
      "Batch [291/1224], Loss: 0.012578536756336689\n",
      "Batch [292/1224], Loss: 0.014995944686233997\n",
      "Batch [293/1224], Loss: 0.012769641354680061\n",
      "Batch [294/1224], Loss: 0.01459516305476427\n",
      "Batch [295/1224], Loss: 0.01706286147236824\n",
      "Batch [296/1224], Loss: 0.014169578440487385\n",
      "Batch [297/1224], Loss: 0.015861447900533676\n",
      "Batch [298/1224], Loss: 0.014482427388429642\n",
      "Batch [299/1224], Loss: 0.013536381535232067\n",
      "Batch [300/1224], Loss: 0.016696995124220848\n",
      "Batch [301/1224], Loss: 0.016080252826213837\n",
      "Batch [302/1224], Loss: 0.014718716032803059\n",
      "Batch [303/1224], Loss: 0.013553047552704811\n",
      "Batch [304/1224], Loss: 0.013354824855923653\n",
      "Batch [305/1224], Loss: 0.012954391539096832\n",
      "Batch [306/1224], Loss: 0.015826143324375153\n",
      "Batch [307/1224], Loss: 0.017008168622851372\n",
      "Batch [308/1224], Loss: 0.015569215640425682\n",
      "Batch [309/1224], Loss: 0.013744039461016655\n",
      "Batch [310/1224], Loss: 0.01428520493209362\n",
      "Batch [311/1224], Loss: 0.015510116703808308\n",
      "Batch [312/1224], Loss: 0.01530451886355877\n",
      "Batch [313/1224], Loss: 0.016420263797044754\n",
      "Batch [314/1224], Loss: 0.014944800175726414\n",
      "Batch [315/1224], Loss: 0.0167512446641922\n",
      "Batch [316/1224], Loss: 0.013547234237194061\n",
      "Batch [317/1224], Loss: 0.014561115764081478\n",
      "Batch [318/1224], Loss: 0.015501217916607857\n",
      "Batch [319/1224], Loss: 0.015063147991895676\n",
      "Batch [320/1224], Loss: 0.014203284867107868\n",
      "Batch [321/1224], Loss: 0.014454086311161518\n",
      "Batch [322/1224], Loss: 0.013450061902403831\n",
      "Batch [323/1224], Loss: 0.01570283994078636\n",
      "Batch [324/1224], Loss: 0.014675321988761425\n",
      "Batch [325/1224], Loss: 0.012630644254386425\n",
      "Batch [326/1224], Loss: 0.013165552169084549\n",
      "Batch [327/1224], Loss: 0.017683500424027443\n",
      "Batch [328/1224], Loss: 0.01342703215777874\n",
      "Batch [329/1224], Loss: 0.014457792975008488\n",
      "Batch [330/1224], Loss: 0.015471572056412697\n",
      "Batch [331/1224], Loss: 0.01199654396623373\n",
      "Batch [332/1224], Loss: 0.015608537942171097\n",
      "Batch [333/1224], Loss: 0.014827211387455463\n",
      "Batch [334/1224], Loss: 0.012827988713979721\n",
      "Batch [335/1224], Loss: 0.016637735068798065\n",
      "Batch [336/1224], Loss: 0.012627036310732365\n",
      "Batch [337/1224], Loss: 0.013618631288409233\n",
      "Batch [338/1224], Loss: 0.014871913008391857\n",
      "Batch [339/1224], Loss: 0.016148190945386887\n",
      "Batch [340/1224], Loss: 0.013452190905809402\n",
      "Batch [341/1224], Loss: 0.011987481266260147\n",
      "Batch [342/1224], Loss: 0.013199658133089542\n",
      "Batch [343/1224], Loss: 0.01528842095285654\n",
      "Batch [344/1224], Loss: 0.012938403524458408\n",
      "Batch [345/1224], Loss: 0.014558709226548672\n",
      "Batch [346/1224], Loss: 0.013005808927118778\n",
      "Batch [347/1224], Loss: 0.012429366819560528\n",
      "Batch [348/1224], Loss: 0.011595104821026325\n",
      "Batch [349/1224], Loss: 0.012722493149340153\n",
      "Batch [350/1224], Loss: 0.013731571845710278\n",
      "Batch [351/1224], Loss: 0.015664534643292427\n",
      "Batch [352/1224], Loss: 0.015262472443282604\n",
      "Batch [353/1224], Loss: 0.01498052291572094\n",
      "Batch [354/1224], Loss: 0.014597916975617409\n",
      "Batch [355/1224], Loss: 0.014208476059138775\n",
      "Batch [356/1224], Loss: 0.014058425091207027\n",
      "Batch [357/1224], Loss: 0.012990815564990044\n",
      "Batch [358/1224], Loss: 0.013582218438386917\n",
      "Batch [359/1224], Loss: 0.01405314914882183\n",
      "Batch [360/1224], Loss: 0.01227667834609747\n",
      "Batch [361/1224], Loss: 0.014384915120899677\n",
      "Batch [362/1224], Loss: 0.015456276945769787\n",
      "Batch [363/1224], Loss: 0.016811545938253403\n",
      "Batch [364/1224], Loss: 0.01337452232837677\n",
      "Batch [365/1224], Loss: 0.013109704479575157\n",
      "Batch [366/1224], Loss: 0.011931836605072021\n",
      "Batch [367/1224], Loss: 0.013378791511058807\n",
      "Batch [368/1224], Loss: 0.01572517305612564\n",
      "Batch [369/1224], Loss: 0.014289972372353077\n",
      "Batch [370/1224], Loss: 0.015489363111555576\n",
      "Batch [371/1224], Loss: 0.011741414666175842\n",
      "Batch [372/1224], Loss: 0.015993690118193626\n",
      "Batch [373/1224], Loss: 0.013178816065192223\n",
      "Batch [374/1224], Loss: 0.017714640125632286\n",
      "Batch [375/1224], Loss: 0.013362079858779907\n",
      "Batch [376/1224], Loss: 0.015605760738253593\n",
      "Batch [377/1224], Loss: 0.013905473984777927\n",
      "Batch [378/1224], Loss: 0.012666323222219944\n",
      "Batch [379/1224], Loss: 0.015255053527653217\n",
      "Batch [380/1224], Loss: 0.013672075234353542\n",
      "Batch [381/1224], Loss: 0.013756156899034977\n",
      "Batch [382/1224], Loss: 0.012969155795872211\n",
      "Batch [383/1224], Loss: 0.012760180048644543\n",
      "Batch [384/1224], Loss: 0.013414801098406315\n",
      "Batch [385/1224], Loss: 0.01626371406018734\n",
      "Batch [386/1224], Loss: 0.01556920912116766\n",
      "Batch [387/1224], Loss: 0.011786475777626038\n",
      "Batch [388/1224], Loss: 0.013071577064692974\n",
      "Batch [389/1224], Loss: 0.014152144081890583\n",
      "Batch [390/1224], Loss: 0.01527970191091299\n",
      "Batch [391/1224], Loss: 0.015526044182479382\n",
      "Batch [392/1224], Loss: 0.015650957822799683\n",
      "Batch [393/1224], Loss: 0.013381152413785458\n",
      "Batch [394/1224], Loss: 0.014256209135055542\n",
      "Batch [395/1224], Loss: 0.014028494246304035\n",
      "Batch [396/1224], Loss: 0.012592964805662632\n",
      "Batch [397/1224], Loss: 0.012696566060185432\n",
      "Batch [398/1224], Loss: 0.011468249373137951\n",
      "Batch [399/1224], Loss: 0.013832127675414085\n",
      "Batch [400/1224], Loss: 0.014303823001682758\n",
      "Batch [401/1224], Loss: 0.016630006954073906\n",
      "Batch [402/1224], Loss: 0.014950635842978954\n",
      "Batch [403/1224], Loss: 0.016856975853443146\n",
      "Batch [404/1224], Loss: 0.016048382967710495\n",
      "Batch [405/1224], Loss: 0.014431793242692947\n",
      "Batch [406/1224], Loss: 0.013788806274533272\n",
      "Batch [407/1224], Loss: 0.014155990444123745\n",
      "Batch [408/1224], Loss: 0.015230609104037285\n",
      "Batch [409/1224], Loss: 0.013248546980321407\n",
      "Batch [410/1224], Loss: 0.014776945114135742\n",
      "Batch [411/1224], Loss: 0.01565966196358204\n",
      "Batch [412/1224], Loss: 0.015545939095318317\n",
      "Batch [413/1224], Loss: 0.015498482622206211\n",
      "Batch [414/1224], Loss: 0.01437807735055685\n",
      "Batch [415/1224], Loss: 0.012131886556744576\n",
      "Batch [416/1224], Loss: 0.0160662941634655\n",
      "Batch [417/1224], Loss: 0.012590426951646805\n",
      "Batch [418/1224], Loss: 0.014110550284385681\n",
      "Batch [419/1224], Loss: 0.012717895209789276\n",
      "Batch [420/1224], Loss: 0.013188114389777184\n",
      "Batch [421/1224], Loss: 0.017141317948698997\n",
      "Batch [422/1224], Loss: 0.01592491939663887\n",
      "Batch [423/1224], Loss: 0.014140450395643711\n",
      "Batch [424/1224], Loss: 0.013720536604523659\n",
      "Batch [425/1224], Loss: 0.016129054129123688\n",
      "Batch [426/1224], Loss: 0.01298459991812706\n",
      "Batch [427/1224], Loss: 0.013934320770204067\n",
      "Batch [428/1224], Loss: 0.015211507678031921\n",
      "Batch [429/1224], Loss: 0.015235302969813347\n",
      "Batch [430/1224], Loss: 0.013411914929747581\n",
      "Batch [431/1224], Loss: 0.013761160895228386\n",
      "Batch [432/1224], Loss: 0.01413958240300417\n",
      "Batch [433/1224], Loss: 0.014948183670639992\n",
      "Batch [434/1224], Loss: 0.013700680807232857\n",
      "Batch [435/1224], Loss: 0.014501365832984447\n",
      "Batch [436/1224], Loss: 0.01500808447599411\n",
      "Batch [437/1224], Loss: 0.014163423329591751\n",
      "Batch [438/1224], Loss: 0.014873328618705273\n",
      "Batch [439/1224], Loss: 0.013449150137603283\n",
      "Batch [440/1224], Loss: 0.013883465901017189\n",
      "Batch [441/1224], Loss: 0.014564020559191704\n",
      "Batch [442/1224], Loss: 0.015413195826113224\n",
      "Batch [443/1224], Loss: 0.016084741801023483\n",
      "Batch [444/1224], Loss: 0.014487767592072487\n",
      "Batch [445/1224], Loss: 0.014576523564755917\n",
      "Batch [446/1224], Loss: 0.014749215915799141\n",
      "Batch [447/1224], Loss: 0.013796962797641754\n",
      "Batch [448/1224], Loss: 0.015730267390608788\n",
      "Batch [449/1224], Loss: 0.012850243598222733\n",
      "Batch [450/1224], Loss: 0.014656735584139824\n",
      "Batch [451/1224], Loss: 0.01277650985866785\n",
      "Batch [452/1224], Loss: 0.016596611589193344\n",
      "Batch [453/1224], Loss: 0.013484849594533443\n",
      "Batch [454/1224], Loss: 0.014406614005565643\n",
      "Batch [455/1224], Loss: 0.013246621005237103\n",
      "Batch [456/1224], Loss: 0.015265574678778648\n",
      "Batch [457/1224], Loss: 0.01841353252530098\n",
      "Batch [458/1224], Loss: 0.01281659584492445\n",
      "Batch [459/1224], Loss: 0.011717448942363262\n",
      "Batch [460/1224], Loss: 0.016924064606428146\n",
      "Batch [461/1224], Loss: 0.013984018005430698\n",
      "Batch [462/1224], Loss: 0.014341622591018677\n",
      "Batch [463/1224], Loss: 0.013190530240535736\n",
      "Batch [464/1224], Loss: 0.013587652705609798\n",
      "Batch [465/1224], Loss: 0.011724850162863731\n",
      "Batch [466/1224], Loss: 0.015020967461168766\n",
      "Batch [467/1224], Loss: 0.012010135687887669\n",
      "Batch [468/1224], Loss: 0.011326642706990242\n",
      "Batch [469/1224], Loss: 0.015369630418717861\n",
      "Batch [470/1224], Loss: 0.013536520302295685\n",
      "Batch [471/1224], Loss: 0.016205012798309326\n",
      "Batch [472/1224], Loss: 0.011954769492149353\n",
      "Batch [473/1224], Loss: 0.014902477152645588\n",
      "Batch [474/1224], Loss: 0.013902915641665459\n",
      "Batch [475/1224], Loss: 0.01478214655071497\n",
      "Batch [476/1224], Loss: 0.014026707969605923\n",
      "Batch [477/1224], Loss: 0.01404044684022665\n",
      "Batch [478/1224], Loss: 0.012377179227769375\n",
      "Batch [479/1224], Loss: 0.014952650293707848\n",
      "Batch [480/1224], Loss: 0.012648068368434906\n",
      "Batch [481/1224], Loss: 0.013490469194948673\n",
      "Batch [482/1224], Loss: 0.01599961705505848\n",
      "Batch [483/1224], Loss: 0.014801601879298687\n",
      "Batch [484/1224], Loss: 0.012094343081116676\n",
      "Batch [485/1224], Loss: 0.01211459655314684\n",
      "Batch [486/1224], Loss: 0.013203212060034275\n",
      "Batch [487/1224], Loss: 0.015624089166522026\n",
      "Batch [488/1224], Loss: 0.014601204544305801\n",
      "Batch [489/1224], Loss: 0.015621937811374664\n",
      "Batch [490/1224], Loss: 0.015352725051343441\n",
      "Batch [491/1224], Loss: 0.015530994161963463\n",
      "Batch [492/1224], Loss: 0.01424876693636179\n",
      "Batch [493/1224], Loss: 0.014864781871438026\n",
      "Batch [494/1224], Loss: 0.014291374012827873\n",
      "Batch [495/1224], Loss: 0.015051369555294514\n",
      "Batch [496/1224], Loss: 0.014051662757992744\n",
      "Batch [497/1224], Loss: 0.01702909916639328\n",
      "Batch [498/1224], Loss: 0.01581060141324997\n",
      "Batch [499/1224], Loss: 0.015768317505717278\n",
      "Batch [500/1224], Loss: 0.015231120400130749\n",
      "Batch [501/1224], Loss: 0.013405529782176018\n",
      "Batch [502/1224], Loss: 0.01240360178053379\n",
      "Batch [503/1224], Loss: 0.015435857698321342\n",
      "Batch [504/1224], Loss: 0.01490706019103527\n",
      "Batch [505/1224], Loss: 0.013832516968250275\n",
      "Batch [506/1224], Loss: 0.01634712517261505\n",
      "Batch [507/1224], Loss: 0.01354456227272749\n",
      "Batch [508/1224], Loss: 0.015432032756507397\n",
      "Batch [509/1224], Loss: 0.01380834449082613\n",
      "Batch [510/1224], Loss: 0.014082902111113071\n",
      "Batch [511/1224], Loss: 0.013071195222437382\n",
      "Batch [512/1224], Loss: 0.014691215939819813\n",
      "Batch [513/1224], Loss: 0.016381150111556053\n",
      "Batch [514/1224], Loss: 0.014292486943304539\n",
      "Batch [515/1224], Loss: 0.014322958886623383\n",
      "Batch [516/1224], Loss: 0.013600576668977737\n",
      "Batch [517/1224], Loss: 0.011569373309612274\n",
      "Batch [518/1224], Loss: 0.01574128307402134\n",
      "Batch [519/1224], Loss: 0.012884831987321377\n",
      "Batch [520/1224], Loss: 0.01545846089720726\n",
      "Batch [521/1224], Loss: 0.014677628874778748\n",
      "Batch [522/1224], Loss: 0.012882575392723083\n",
      "Batch [523/1224], Loss: 0.013555316254496574\n",
      "Batch [524/1224], Loss: 0.016381707042455673\n",
      "Batch [525/1224], Loss: 0.014029980637133121\n",
      "Batch [526/1224], Loss: 0.012533243745565414\n",
      "Batch [527/1224], Loss: 0.01667674072086811\n",
      "Batch [528/1224], Loss: 0.013815654441714287\n",
      "Batch [529/1224], Loss: 0.015628468245267868\n",
      "Batch [530/1224], Loss: 0.01689908280968666\n",
      "Batch [531/1224], Loss: 0.01649005152285099\n",
      "Batch [532/1224], Loss: 0.014731461182236671\n",
      "Batch [533/1224], Loss: 0.015020981431007385\n",
      "Batch [534/1224], Loss: 0.01354601513594389\n",
      "Batch [535/1224], Loss: 0.013477945700287819\n",
      "Batch [536/1224], Loss: 0.013424650765955448\n",
      "Batch [537/1224], Loss: 0.013950993306934834\n",
      "Batch [538/1224], Loss: 0.012518810108304024\n",
      "Batch [539/1224], Loss: 0.016052691265940666\n",
      "Batch [540/1224], Loss: 0.01371874287724495\n",
      "Batch [541/1224], Loss: 0.013086381368339062\n",
      "Batch [542/1224], Loss: 0.014967727474868298\n",
      "Batch [543/1224], Loss: 0.013642240315675735\n",
      "Batch [544/1224], Loss: 0.016756996512413025\n",
      "Batch [545/1224], Loss: 0.016072018072009087\n",
      "Batch [546/1224], Loss: 0.012962608598172665\n",
      "Batch [547/1224], Loss: 0.014719872735440731\n",
      "Batch [548/1224], Loss: 0.015077637508511543\n",
      "Batch [549/1224], Loss: 0.018169185146689415\n",
      "Batch [550/1224], Loss: 0.013169507496058941\n",
      "Batch [551/1224], Loss: 0.014737807214260101\n",
      "Batch [552/1224], Loss: 0.01378557924181223\n",
      "Batch [553/1224], Loss: 0.012118212878704071\n",
      "Batch [554/1224], Loss: 0.013206349685788155\n",
      "Batch [555/1224], Loss: 0.012758352793753147\n",
      "Batch [556/1224], Loss: 0.014639750123023987\n",
      "Batch [557/1224], Loss: 0.014067240990698338\n",
      "Batch [558/1224], Loss: 0.016559848561882973\n",
      "Batch [559/1224], Loss: 0.01378110982477665\n",
      "Batch [560/1224], Loss: 0.012473098933696747\n",
      "Batch [561/1224], Loss: 0.014862703159451485\n",
      "Batch [562/1224], Loss: 0.012742118909955025\n",
      "Batch [563/1224], Loss: 0.013162467628717422\n",
      "Batch [564/1224], Loss: 0.015160790644586086\n",
      "Batch [565/1224], Loss: 0.013188375160098076\n",
      "Batch [566/1224], Loss: 0.01457308977842331\n",
      "Batch [567/1224], Loss: 0.014453654177486897\n",
      "Batch [568/1224], Loss: 0.014356407336890697\n",
      "Batch [569/1224], Loss: 0.015460601076483727\n",
      "Batch [570/1224], Loss: 0.01218885462731123\n",
      "Batch [571/1224], Loss: 0.01433893945068121\n",
      "Batch [572/1224], Loss: 0.015774723142385483\n",
      "Batch [573/1224], Loss: 0.014343746937811375\n",
      "Batch [574/1224], Loss: 0.015125533565878868\n",
      "Batch [575/1224], Loss: 0.013261519372463226\n",
      "Batch [576/1224], Loss: 0.015022361651062965\n",
      "Batch [577/1224], Loss: 0.01370309293270111\n",
      "Batch [578/1224], Loss: 0.01295736338943243\n",
      "Batch [579/1224], Loss: 0.016131991520524025\n",
      "Batch [580/1224], Loss: 0.014499114826321602\n",
      "Batch [581/1224], Loss: 0.012586689554154873\n",
      "Batch [582/1224], Loss: 0.011843849904835224\n",
      "Batch [583/1224], Loss: 0.013330036774277687\n",
      "Batch [584/1224], Loss: 0.01484723575413227\n",
      "Batch [585/1224], Loss: 0.011815205216407776\n",
      "Batch [586/1224], Loss: 0.015166714787483215\n",
      "Batch [587/1224], Loss: 0.013975733891129494\n",
      "Batch [588/1224], Loss: 0.011733287945389748\n",
      "Batch [589/1224], Loss: 0.014947524294257164\n",
      "Batch [590/1224], Loss: 0.013215201906859875\n",
      "Batch [591/1224], Loss: 0.016721956431865692\n",
      "Batch [592/1224], Loss: 0.013400756753981113\n",
      "Batch [593/1224], Loss: 0.013806312344968319\n",
      "Batch [594/1224], Loss: 0.014343811199069023\n",
      "Batch [595/1224], Loss: 0.014576717279851437\n",
      "Batch [596/1224], Loss: 0.013236706145107746\n",
      "Batch [597/1224], Loss: 0.013057271018624306\n",
      "Batch [598/1224], Loss: 0.014517930336296558\n",
      "Batch [599/1224], Loss: 0.012195929884910583\n",
      "Batch [600/1224], Loss: 0.015738699585199356\n",
      "Batch [601/1224], Loss: 0.0137140192091465\n",
      "Batch [602/1224], Loss: 0.012772811576724052\n",
      "Batch [603/1224], Loss: 0.012501295655965805\n",
      "Batch [604/1224], Loss: 0.013036110438406467\n",
      "Batch [605/1224], Loss: 0.013555162586271763\n",
      "Batch [606/1224], Loss: 0.015318323858082294\n",
      "Batch [607/1224], Loss: 0.014598770998418331\n",
      "Batch [608/1224], Loss: 0.01659337803721428\n",
      "Batch [609/1224], Loss: 0.013211233541369438\n",
      "Batch [610/1224], Loss: 0.014489117078483105\n",
      "Batch [611/1224], Loss: 0.012806818820536137\n",
      "Batch [612/1224], Loss: 0.013627784326672554\n",
      "Batch [613/1224], Loss: 0.01316944882273674\n",
      "Batch [614/1224], Loss: 0.013932634145021439\n",
      "Batch [615/1224], Loss: 0.012647694908082485\n",
      "Batch [616/1224], Loss: 0.015825442969799042\n",
      "Batch [617/1224], Loss: 0.013791633769869804\n",
      "Batch [618/1224], Loss: 0.012998746708035469\n",
      "Batch [619/1224], Loss: 0.015168514102697372\n",
      "Batch [620/1224], Loss: 0.014361578039824963\n",
      "Batch [621/1224], Loss: 0.013776184991002083\n",
      "Batch [622/1224], Loss: 0.015353042632341385\n",
      "Batch [623/1224], Loss: 0.014895141124725342\n",
      "Batch [624/1224], Loss: 0.011050090193748474\n",
      "Batch [625/1224], Loss: 0.014385106973350048\n",
      "Batch [626/1224], Loss: 0.015450824052095413\n",
      "Batch [627/1224], Loss: 0.016804205253720284\n",
      "Batch [628/1224], Loss: 0.01621350459754467\n",
      "Batch [629/1224], Loss: 0.01493027713149786\n",
      "Batch [630/1224], Loss: 0.017426060512661934\n",
      "Batch [631/1224], Loss: 0.012060906738042831\n",
      "Batch [632/1224], Loss: 0.015019780024886131\n",
      "Batch [633/1224], Loss: 0.016621803864836693\n",
      "Batch [634/1224], Loss: 0.014579663053154945\n",
      "Batch [635/1224], Loss: 0.014118045568466187\n",
      "Batch [636/1224], Loss: 0.014320162124931812\n",
      "Batch [637/1224], Loss: 0.015013707801699638\n",
      "Batch [638/1224], Loss: 0.013253127224743366\n",
      "Batch [639/1224], Loss: 0.015160312876105309\n",
      "Batch [640/1224], Loss: 0.014787563122808933\n",
      "Batch [641/1224], Loss: 0.014990286901593208\n",
      "Batch [642/1224], Loss: 0.015448512509465218\n",
      "Batch [643/1224], Loss: 0.014759860932826996\n",
      "Batch [644/1224], Loss: 0.0141059011220932\n",
      "Batch [645/1224], Loss: 0.014363043010234833\n",
      "Batch [646/1224], Loss: 0.01499752327799797\n",
      "Batch [647/1224], Loss: 0.016064440831542015\n",
      "Batch [648/1224], Loss: 0.012243620119988918\n",
      "Batch [649/1224], Loss: 0.014427452348172665\n",
      "Batch [650/1224], Loss: 0.014475584961473942\n",
      "Batch [651/1224], Loss: 0.012660941109061241\n",
      "Batch [652/1224], Loss: 0.013927767053246498\n",
      "Batch [653/1224], Loss: 0.014579281210899353\n",
      "Batch [654/1224], Loss: 0.013422704301774502\n",
      "Batch [655/1224], Loss: 0.015225745737552643\n",
      "Batch [656/1224], Loss: 0.0157717764377594\n",
      "Batch [657/1224], Loss: 0.014777651987969875\n",
      "Batch [658/1224], Loss: 0.015666067600250244\n",
      "Batch [659/1224], Loss: 0.013919890858232975\n",
      "Batch [660/1224], Loss: 0.01573900692164898\n",
      "Batch [661/1224], Loss: 0.01298054214566946\n",
      "Batch [662/1224], Loss: 0.013937149196863174\n",
      "Batch [663/1224], Loss: 0.012229755520820618\n",
      "Batch [664/1224], Loss: 0.01514274999499321\n",
      "Batch [665/1224], Loss: 0.012258914299309254\n",
      "Batch [666/1224], Loss: 0.01249296497553587\n",
      "Batch [667/1224], Loss: 0.014603009447455406\n",
      "Batch [668/1224], Loss: 0.013038270175457\n",
      "Batch [669/1224], Loss: 0.017019782215356827\n",
      "Batch [670/1224], Loss: 0.013660834170877934\n",
      "Batch [671/1224], Loss: 0.015171892940998077\n",
      "Batch [672/1224], Loss: 0.013472236692905426\n",
      "Batch [673/1224], Loss: 0.015074966475367546\n",
      "Batch [674/1224], Loss: 0.014939505606889725\n",
      "Batch [675/1224], Loss: 0.014154691249132156\n",
      "Batch [676/1224], Loss: 0.01604774408042431\n",
      "Batch [677/1224], Loss: 0.01338090281933546\n",
      "Batch [678/1224], Loss: 0.013776508159935474\n",
      "Batch [679/1224], Loss: 0.014225485734641552\n",
      "Batch [680/1224], Loss: 0.013483274728059769\n",
      "Batch [681/1224], Loss: 0.012992854230105877\n",
      "Batch [682/1224], Loss: 0.016418687999248505\n",
      "Batch [683/1224], Loss: 0.015947526320815086\n",
      "Batch [684/1224], Loss: 0.01434409897774458\n",
      "Batch [685/1224], Loss: 0.01538532879203558\n",
      "Batch [686/1224], Loss: 0.013554253615438938\n",
      "Batch [687/1224], Loss: 0.015468000434339046\n",
      "Batch [688/1224], Loss: 0.01541908085346222\n",
      "Batch [689/1224], Loss: 0.015068866312503815\n",
      "Batch [690/1224], Loss: 0.012981981039047241\n",
      "Batch [691/1224], Loss: 0.015926018357276917\n",
      "Batch [692/1224], Loss: 0.013669230043888092\n",
      "Batch [693/1224], Loss: 0.013248732313513756\n",
      "Batch [694/1224], Loss: 0.016270514577627182\n",
      "Batch [695/1224], Loss: 0.0146007826551795\n",
      "Batch [696/1224], Loss: 0.015938252210617065\n",
      "Batch [697/1224], Loss: 0.012607833370566368\n",
      "Batch [698/1224], Loss: 0.012728346511721611\n",
      "Batch [699/1224], Loss: 0.012607908807694912\n",
      "Batch [700/1224], Loss: 0.011828353628516197\n",
      "Batch [701/1224], Loss: 0.012857148423790932\n",
      "Batch [702/1224], Loss: 0.014980394393205643\n",
      "Batch [703/1224], Loss: 0.011435399763286114\n",
      "Batch [704/1224], Loss: 0.011577541008591652\n",
      "Batch [705/1224], Loss: 0.013366553001105785\n",
      "Batch [706/1224], Loss: 0.011727632954716682\n",
      "Batch [707/1224], Loss: 0.014281345531344414\n",
      "Batch [708/1224], Loss: 0.013225637376308441\n",
      "Batch [709/1224], Loss: 0.014136898331344128\n",
      "Batch [710/1224], Loss: 0.01449604146182537\n",
      "Batch [711/1224], Loss: 0.012666011229157448\n",
      "Batch [712/1224], Loss: 0.01463155634701252\n",
      "Batch [713/1224], Loss: 0.014529186300933361\n",
      "Batch [714/1224], Loss: 0.017206981778144836\n",
      "Batch [715/1224], Loss: 0.01455264538526535\n",
      "Batch [716/1224], Loss: 0.015213141217827797\n",
      "Batch [717/1224], Loss: 0.014327453449368477\n",
      "Batch [718/1224], Loss: 0.011995673179626465\n",
      "Batch [719/1224], Loss: 0.015357011929154396\n",
      "Batch [720/1224], Loss: 0.015905797481536865\n",
      "Batch [721/1224], Loss: 0.012489007785916328\n",
      "Batch [722/1224], Loss: 0.015272445045411587\n",
      "Batch [723/1224], Loss: 0.01586735248565674\n",
      "Batch [724/1224], Loss: 0.015128431841731071\n",
      "Batch [725/1224], Loss: 0.014017441309988499\n",
      "Batch [726/1224], Loss: 0.01473412849009037\n",
      "Batch [727/1224], Loss: 0.014390699565410614\n",
      "Batch [728/1224], Loss: 0.013671386986970901\n",
      "Batch [729/1224], Loss: 0.01490513700991869\n",
      "Batch [730/1224], Loss: 0.01638069748878479\n",
      "Batch [731/1224], Loss: 0.013374434784054756\n",
      "Batch [732/1224], Loss: 0.012753762304782867\n",
      "Batch [733/1224], Loss: 0.012232646346092224\n",
      "Batch [734/1224], Loss: 0.014291820116341114\n",
      "Batch [735/1224], Loss: 0.010691232979297638\n",
      "Batch [736/1224], Loss: 0.015006769448518753\n",
      "Batch [737/1224], Loss: 0.015505960211157799\n",
      "Batch [738/1224], Loss: 0.014214294031262398\n",
      "Batch [739/1224], Loss: 0.015494851395487785\n",
      "Batch [740/1224], Loss: 0.015887588262557983\n",
      "Batch [741/1224], Loss: 0.012576306238770485\n",
      "Batch [742/1224], Loss: 0.012322432361543179\n",
      "Batch [743/1224], Loss: 0.013817161321640015\n",
      "Batch [744/1224], Loss: 0.014101703651249409\n",
      "Batch [745/1224], Loss: 0.014809888787567616\n",
      "Batch [746/1224], Loss: 0.013283983804285526\n",
      "Batch [747/1224], Loss: 0.014058133587241173\n",
      "Batch [748/1224], Loss: 0.014033437706530094\n",
      "Batch [749/1224], Loss: 0.015266002155840397\n",
      "Batch [750/1224], Loss: 0.014115982688963413\n",
      "Batch [751/1224], Loss: 0.013804594986140728\n",
      "Batch [752/1224], Loss: 0.01386599987745285\n",
      "Batch [753/1224], Loss: 0.012853527441620827\n",
      "Batch [754/1224], Loss: 0.014721482060849667\n",
      "Batch [755/1224], Loss: 0.015325712040066719\n",
      "Batch [756/1224], Loss: 0.01450639683753252\n",
      "Batch [757/1224], Loss: 0.012094170786440372\n",
      "Batch [758/1224], Loss: 0.015895593911409378\n",
      "Batch [759/1224], Loss: 0.014063523150980473\n",
      "Batch [760/1224], Loss: 0.01605853997170925\n",
      "Batch [761/1224], Loss: 0.013254557736217976\n",
      "Batch [762/1224], Loss: 0.013745889067649841\n",
      "Batch [763/1224], Loss: 0.013352049514651299\n",
      "Batch [764/1224], Loss: 0.015115887857973576\n",
      "Batch [765/1224], Loss: 0.016385771334171295\n",
      "Batch [766/1224], Loss: 0.012134402059018612\n",
      "Batch [767/1224], Loss: 0.015111880376935005\n",
      "Batch [768/1224], Loss: 0.015298806130886078\n",
      "Batch [769/1224], Loss: 0.01599658466875553\n",
      "Batch [770/1224], Loss: 0.01397243607789278\n",
      "Batch [771/1224], Loss: 0.015495860949158669\n",
      "Batch [772/1224], Loss: 0.015346926636993885\n",
      "Batch [773/1224], Loss: 0.014051491394639015\n",
      "Batch [774/1224], Loss: 0.013222819194197655\n",
      "Batch [775/1224], Loss: 0.014788515865802765\n",
      "Batch [776/1224], Loss: 0.01653902791440487\n",
      "Batch [777/1224], Loss: 0.01470375619828701\n",
      "Batch [778/1224], Loss: 0.010950806550681591\n",
      "Batch [779/1224], Loss: 0.01446186751127243\n",
      "Batch [780/1224], Loss: 0.014897632412612438\n",
      "Batch [781/1224], Loss: 0.012774260714650154\n",
      "Batch [782/1224], Loss: 0.014520497061312199\n",
      "Batch [783/1224], Loss: 0.014822892844676971\n",
      "Batch [784/1224], Loss: 0.01670285128057003\n",
      "Batch [785/1224], Loss: 0.016423067077994347\n",
      "Batch [786/1224], Loss: 0.015288702212274075\n",
      "Batch [787/1224], Loss: 0.013628249056637287\n",
      "Batch [788/1224], Loss: 0.014471575617790222\n",
      "Batch [789/1224], Loss: 0.01587771438062191\n",
      "Batch [790/1224], Loss: 0.014713847078382969\n",
      "Batch [791/1224], Loss: 0.014528426341712475\n",
      "Batch [792/1224], Loss: 0.014120719395577908\n",
      "Batch [793/1224], Loss: 0.014115085825324059\n",
      "Batch [794/1224], Loss: 0.014526210725307465\n",
      "Batch [795/1224], Loss: 0.014238199219107628\n",
      "Batch [796/1224], Loss: 0.014825781807303429\n",
      "Batch [797/1224], Loss: 0.013867645524442196\n",
      "Batch [798/1224], Loss: 0.014528980478644371\n",
      "Batch [799/1224], Loss: 0.01381683349609375\n",
      "Batch [800/1224], Loss: 0.014390050433576107\n",
      "Batch [801/1224], Loss: 0.014307307079434395\n",
      "Batch [802/1224], Loss: 0.012721185572445393\n",
      "Batch [803/1224], Loss: 0.013642015866935253\n",
      "Batch [804/1224], Loss: 0.01212478056550026\n",
      "Batch [805/1224], Loss: 0.015024950727820396\n",
      "Batch [806/1224], Loss: 0.013820335268974304\n",
      "Batch [807/1224], Loss: 0.013858934864401817\n",
      "Batch [808/1224], Loss: 0.017571087926626205\n",
      "Batch [809/1224], Loss: 0.012379386462271214\n",
      "Batch [810/1224], Loss: 0.01717742718756199\n",
      "Batch [811/1224], Loss: 0.016044974327087402\n",
      "Batch [812/1224], Loss: 0.01316501758992672\n",
      "Batch [813/1224], Loss: 0.015282854437828064\n",
      "Batch [814/1224], Loss: 0.013846690766513348\n",
      "Batch [815/1224], Loss: 0.013133857399225235\n",
      "Batch [816/1224], Loss: 0.014262543991208076\n",
      "Batch [817/1224], Loss: 0.0134345181286335\n",
      "Batch [818/1224], Loss: 0.013564549386501312\n",
      "Batch [819/1224], Loss: 0.0166370440274477\n",
      "Batch [820/1224], Loss: 0.01366055104881525\n",
      "Batch [821/1224], Loss: 0.015101837925612926\n",
      "Batch [822/1224], Loss: 0.015310818329453468\n",
      "Batch [823/1224], Loss: 0.012337973341345787\n",
      "Batch [824/1224], Loss: 0.013183421455323696\n",
      "Batch [825/1224], Loss: 0.013750866055488586\n",
      "Batch [826/1224], Loss: 0.012740310281515121\n",
      "Batch [827/1224], Loss: 0.014395463280379772\n",
      "Batch [828/1224], Loss: 0.013970613479614258\n",
      "Batch [829/1224], Loss: 0.015637557953596115\n",
      "Batch [830/1224], Loss: 0.01655578427016735\n",
      "Batch [831/1224], Loss: 0.015348128974437714\n",
      "Batch [832/1224], Loss: 0.014049114659428596\n",
      "Batch [833/1224], Loss: 0.013212373480200768\n",
      "Batch [834/1224], Loss: 0.01284888293594122\n",
      "Batch [835/1224], Loss: 0.012162119150161743\n",
      "Batch [836/1224], Loss: 0.016340574249625206\n",
      "Batch [837/1224], Loss: 0.014317430555820465\n",
      "Batch [838/1224], Loss: 0.01577235572040081\n",
      "Batch [839/1224], Loss: 0.012879973277449608\n",
      "Batch [840/1224], Loss: 0.01263249758630991\n",
      "Batch [841/1224], Loss: 0.014289631508290768\n",
      "Batch [842/1224], Loss: 0.013509348034858704\n",
      "Batch [843/1224], Loss: 0.014766564592719078\n",
      "Batch [844/1224], Loss: 0.013278606347739697\n",
      "Batch [845/1224], Loss: 0.015086853876709938\n",
      "Batch [846/1224], Loss: 0.014031886123120785\n",
      "Batch [847/1224], Loss: 0.01606345735490322\n",
      "Batch [848/1224], Loss: 0.012778091244399548\n",
      "Batch [849/1224], Loss: 0.013161804527044296\n",
      "Batch [850/1224], Loss: 0.014734672382473946\n",
      "Batch [851/1224], Loss: 0.01186276413500309\n",
      "Batch [852/1224], Loss: 0.013371570035815239\n",
      "Batch [853/1224], Loss: 0.012523056007921696\n",
      "Batch [854/1224], Loss: 0.015784481540322304\n",
      "Batch [855/1224], Loss: 0.014561391435563564\n",
      "Batch [856/1224], Loss: 0.014853359200060368\n",
      "Batch [857/1224], Loss: 0.015118480660021305\n",
      "Batch [858/1224], Loss: 0.013399380259215832\n",
      "Batch [859/1224], Loss: 0.013524988666176796\n",
      "Batch [860/1224], Loss: 0.014845753088593483\n",
      "Batch [861/1224], Loss: 0.014118123799562454\n",
      "Batch [862/1224], Loss: 0.01734144054353237\n",
      "Batch [863/1224], Loss: 0.014432687312364578\n",
      "Batch [864/1224], Loss: 0.013650340959429741\n",
      "Batch [865/1224], Loss: 0.015215618535876274\n",
      "Batch [866/1224], Loss: 0.014579812996089458\n",
      "Batch [867/1224], Loss: 0.013066191226243973\n",
      "Batch [868/1224], Loss: 0.013914727605879307\n",
      "Batch [869/1224], Loss: 0.014689808711409569\n",
      "Batch [870/1224], Loss: 0.013730109669268131\n",
      "Batch [871/1224], Loss: 0.012884804047644138\n",
      "Batch [872/1224], Loss: 0.012327931821346283\n",
      "Batch [873/1224], Loss: 0.011670061387121677\n",
      "Batch [874/1224], Loss: 0.011374049820005894\n",
      "Batch [875/1224], Loss: 0.014198679476976395\n",
      "Batch [876/1224], Loss: 0.012637301348149776\n",
      "Batch [877/1224], Loss: 0.012380716390907764\n",
      "Batch [878/1224], Loss: 0.016991009935736656\n",
      "Batch [879/1224], Loss: 0.011699983850121498\n",
      "Batch [880/1224], Loss: 0.012720361351966858\n",
      "Batch [881/1224], Loss: 0.01342856790870428\n",
      "Batch [882/1224], Loss: 0.01374855823814869\n",
      "Batch [883/1224], Loss: 0.01390608586370945\n",
      "Batch [884/1224], Loss: 0.012426300905644894\n",
      "Batch [885/1224], Loss: 0.0163631122559309\n",
      "Batch [886/1224], Loss: 0.012651169672608376\n",
      "Batch [887/1224], Loss: 0.013866163790225983\n",
      "Batch [888/1224], Loss: 0.01294937264174223\n",
      "Batch [889/1224], Loss: 0.0120891397818923\n",
      "Batch [890/1224], Loss: 0.015483386814594269\n",
      "Batch [891/1224], Loss: 0.01319230254739523\n",
      "Batch [892/1224], Loss: 0.011939037591218948\n",
      "Batch [893/1224], Loss: 0.01460734661668539\n",
      "Batch [894/1224], Loss: 0.011929153464734554\n",
      "Batch [895/1224], Loss: 0.01363343559205532\n",
      "Batch [896/1224], Loss: 0.014846296049654484\n",
      "Batch [897/1224], Loss: 0.01372224185615778\n",
      "Batch [898/1224], Loss: 0.01020586583763361\n",
      "Batch [899/1224], Loss: 0.013037416152656078\n",
      "Batch [900/1224], Loss: 0.013441510498523712\n",
      "Batch [901/1224], Loss: 0.01415945589542389\n",
      "Batch [902/1224], Loss: 0.014330501668155193\n",
      "Batch [903/1224], Loss: 0.01380525715649128\n",
      "Batch [904/1224], Loss: 0.012097123079001904\n",
      "Batch [905/1224], Loss: 0.013040708377957344\n",
      "Batch [906/1224], Loss: 0.01282378938049078\n",
      "Batch [907/1224], Loss: 0.01634090766310692\n",
      "Batch [908/1224], Loss: 0.014864835888147354\n",
      "Batch [909/1224], Loss: 0.01275873463600874\n",
      "Batch [910/1224], Loss: 0.013507374562323093\n",
      "Batch [911/1224], Loss: 0.013721897266805172\n",
      "Batch [912/1224], Loss: 0.01293130312114954\n",
      "Batch [913/1224], Loss: 0.013109460473060608\n",
      "Batch [914/1224], Loss: 0.016212420538067818\n",
      "Batch [915/1224], Loss: 0.013358226045966148\n",
      "Batch [916/1224], Loss: 0.01544944941997528\n",
      "Batch [917/1224], Loss: 0.013003732077777386\n",
      "Batch [918/1224], Loss: 0.012814880348742008\n",
      "Batch [919/1224], Loss: 0.013656317256391048\n",
      "Batch [920/1224], Loss: 0.0139694819226861\n",
      "Batch [921/1224], Loss: 0.013965434394776821\n",
      "Batch [922/1224], Loss: 0.01483508013188839\n",
      "Batch [923/1224], Loss: 0.01332968007773161\n",
      "Batch [924/1224], Loss: 0.01374097727239132\n",
      "Batch [925/1224], Loss: 0.017094604671001434\n",
      "Batch [926/1224], Loss: 0.013501245528459549\n",
      "Batch [927/1224], Loss: 0.011876051314175129\n",
      "Batch [928/1224], Loss: 0.01681843213737011\n",
      "Batch [929/1224], Loss: 0.012753479182720184\n",
      "Batch [930/1224], Loss: 0.016278594732284546\n",
      "Batch [931/1224], Loss: 0.013662949204444885\n",
      "Batch [932/1224], Loss: 0.013381615281105042\n",
      "Batch [933/1224], Loss: 0.014380539767444134\n",
      "Batch [934/1224], Loss: 0.013619398698210716\n",
      "Batch [935/1224], Loss: 0.013025220483541489\n",
      "Batch [936/1224], Loss: 0.01648326776921749\n",
      "Batch [937/1224], Loss: 0.013572454452514648\n",
      "Batch [938/1224], Loss: 0.014571820385754108\n",
      "Batch [939/1224], Loss: 0.014681801199913025\n",
      "Batch [940/1224], Loss: 0.014000515453517437\n",
      "Batch [941/1224], Loss: 0.01286257617175579\n",
      "Batch [942/1224], Loss: 0.014992388896644115\n",
      "Batch [943/1224], Loss: 0.013783309608697891\n",
      "Batch [944/1224], Loss: 0.014447404071688652\n",
      "Batch [945/1224], Loss: 0.015451907180249691\n",
      "Batch [946/1224], Loss: 0.01529738213866949\n",
      "Batch [947/1224], Loss: 0.012771490029990673\n",
      "Batch [948/1224], Loss: 0.013994203880429268\n",
      "Batch [949/1224], Loss: 0.013604520820081234\n",
      "Batch [950/1224], Loss: 0.012988472357392311\n",
      "Batch [951/1224], Loss: 0.013452108949422836\n",
      "Batch [952/1224], Loss: 0.01499155629426241\n",
      "Batch [953/1224], Loss: 0.01428569108247757\n",
      "Batch [954/1224], Loss: 0.013393654488027096\n",
      "Batch [955/1224], Loss: 0.010265365242958069\n",
      "Batch [956/1224], Loss: 0.014206771738827229\n",
      "Batch [957/1224], Loss: 0.015432254411280155\n",
      "Batch [958/1224], Loss: 0.01022428646683693\n",
      "Batch [959/1224], Loss: 0.013374035246670246\n",
      "Batch [960/1224], Loss: 0.014694340527057648\n",
      "Batch [961/1224], Loss: 0.0185167845338583\n",
      "Batch [962/1224], Loss: 0.013754102401435375\n",
      "Batch [963/1224], Loss: 0.013946466147899628\n",
      "Batch [964/1224], Loss: 0.015294953249394894\n",
      "Batch [965/1224], Loss: 0.013074937276542187\n",
      "Batch [966/1224], Loss: 0.014691541902720928\n",
      "Batch [967/1224], Loss: 0.01204286515712738\n",
      "Batch [968/1224], Loss: 0.016050826758146286\n",
      "Batch [969/1224], Loss: 0.0159952100366354\n",
      "Batch [970/1224], Loss: 0.013167300261557102\n",
      "Batch [971/1224], Loss: 0.013924776576459408\n",
      "Batch [972/1224], Loss: 0.011029466986656189\n",
      "Batch [973/1224], Loss: 0.015051097609102726\n",
      "Batch [974/1224], Loss: 0.01596607081592083\n",
      "Batch [975/1224], Loss: 0.012350178323686123\n",
      "Batch [976/1224], Loss: 0.01402206253260374\n",
      "Batch [977/1224], Loss: 0.011767786927521229\n",
      "Batch [978/1224], Loss: 0.016766095533967018\n",
      "Batch [979/1224], Loss: 0.015191753394901752\n",
      "Batch [980/1224], Loss: 0.014386896975338459\n",
      "Batch [981/1224], Loss: 0.012844567187130451\n",
      "Batch [982/1224], Loss: 0.011895070783793926\n",
      "Batch [983/1224], Loss: 0.012544489465653896\n",
      "Batch [984/1224], Loss: 0.014121961779892445\n",
      "Batch [985/1224], Loss: 0.016117773950099945\n",
      "Batch [986/1224], Loss: 0.013569682836532593\n",
      "Batch [987/1224], Loss: 0.0131008829921484\n",
      "Batch [988/1224], Loss: 0.013135669752955437\n",
      "Batch [989/1224], Loss: 0.014268210157752037\n",
      "Batch [990/1224], Loss: 0.014065600000321865\n",
      "Batch [991/1224], Loss: 0.011072106659412384\n",
      "Batch [992/1224], Loss: 0.012189814820885658\n",
      "Batch [993/1224], Loss: 0.010920410044491291\n",
      "Batch [994/1224], Loss: 0.015338325873017311\n",
      "Batch [995/1224], Loss: 0.013575010932981968\n",
      "Batch [996/1224], Loss: 0.014254690147936344\n",
      "Batch [997/1224], Loss: 0.010220278054475784\n",
      "Batch [998/1224], Loss: 0.016009682789444923\n",
      "Batch [999/1224], Loss: 0.012775988318026066\n",
      "Batch [1000/1224], Loss: 0.015669183805584908\n",
      "Batch [1001/1224], Loss: 0.016428908333182335\n",
      "Batch [1002/1224], Loss: 0.011696986854076385\n",
      "Batch [1003/1224], Loss: 0.01462717168033123\n",
      "Batch [1004/1224], Loss: 0.01425004843622446\n",
      "Batch [1005/1224], Loss: 0.015351194888353348\n",
      "Batch [1006/1224], Loss: 0.013257107697427273\n",
      "Batch [1007/1224], Loss: 0.014817186631262302\n",
      "Batch [1008/1224], Loss: 0.014834216795861721\n",
      "Batch [1009/1224], Loss: 0.011657548137009144\n",
      "Batch [1010/1224], Loss: 0.015092026442289352\n",
      "Batch [1011/1224], Loss: 0.015429411083459854\n",
      "Batch [1012/1224], Loss: 0.013462225906550884\n",
      "Batch [1013/1224], Loss: 0.0150865213945508\n",
      "Batch [1014/1224], Loss: 0.017498474568128586\n",
      "Batch [1015/1224], Loss: 0.012790017761290073\n",
      "Batch [1016/1224], Loss: 0.014885866083204746\n",
      "Batch [1017/1224], Loss: 0.014064944349229336\n",
      "Batch [1018/1224], Loss: 0.01607017032802105\n",
      "Batch [1019/1224], Loss: 0.0128580117598176\n",
      "Batch [1020/1224], Loss: 0.013812625780701637\n",
      "Batch [1021/1224], Loss: 0.01559674646705389\n",
      "Batch [1022/1224], Loss: 0.015142779797315598\n",
      "Batch [1023/1224], Loss: 0.013567738234996796\n",
      "Batch [1024/1224], Loss: 0.015240936540067196\n",
      "Batch [1025/1224], Loss: 0.012924855574965477\n",
      "Batch [1026/1224], Loss: 0.016657764092087746\n",
      "Batch [1027/1224], Loss: 0.014844718389213085\n",
      "Batch [1028/1224], Loss: 0.013894465751945972\n",
      "Batch [1029/1224], Loss: 0.012640836648643017\n",
      "Batch [1030/1224], Loss: 0.015358075499534607\n",
      "Batch [1031/1224], Loss: 0.014756240881979465\n",
      "Batch [1032/1224], Loss: 0.012028176337480545\n",
      "Batch [1033/1224], Loss: 0.015711238607764244\n",
      "Batch [1034/1224], Loss: 0.01574837602674961\n",
      "Batch [1035/1224], Loss: 0.01325096096843481\n",
      "Batch [1036/1224], Loss: 0.015722138807177544\n",
      "Batch [1037/1224], Loss: 0.013982436619699001\n",
      "Batch [1038/1224], Loss: 0.013939136639237404\n",
      "Batch [1039/1224], Loss: 0.014691379852592945\n",
      "Batch [1040/1224], Loss: 0.013691403903067112\n",
      "Batch [1041/1224], Loss: 0.015570935793220997\n",
      "Batch [1042/1224], Loss: 0.016704613342881203\n",
      "Batch [1043/1224], Loss: 0.011500504799187183\n",
      "Batch [1044/1224], Loss: 0.01600564830005169\n",
      "Batch [1045/1224], Loss: 0.012481408193707466\n",
      "Batch [1046/1224], Loss: 0.015466995537281036\n",
      "Batch [1047/1224], Loss: 0.012959856539964676\n",
      "Batch [1048/1224], Loss: 0.01583055593073368\n",
      "Batch [1049/1224], Loss: 0.01546996459364891\n",
      "Batch [1050/1224], Loss: 0.01569058559834957\n",
      "Batch [1051/1224], Loss: 0.012349649332463741\n",
      "Batch [1052/1224], Loss: 0.013351325877010822\n",
      "Batch [1053/1224], Loss: 0.016972854733467102\n",
      "Batch [1054/1224], Loss: 0.014665096998214722\n",
      "Batch [1055/1224], Loss: 0.014571046456694603\n",
      "Batch [1056/1224], Loss: 0.01229279488325119\n",
      "Batch [1057/1224], Loss: 0.01613457314670086\n",
      "Batch [1058/1224], Loss: 0.012548568658530712\n",
      "Batch [1059/1224], Loss: 0.015579786151647568\n",
      "Batch [1060/1224], Loss: 0.014242083765566349\n",
      "Batch [1061/1224], Loss: 0.014143678359687328\n",
      "Batch [1062/1224], Loss: 0.01336443331092596\n",
      "Batch [1063/1224], Loss: 0.015930891036987305\n",
      "Batch [1064/1224], Loss: 0.013199878856539726\n",
      "Batch [1065/1224], Loss: 0.011334712617099285\n",
      "Batch [1066/1224], Loss: 0.012629439122974873\n",
      "Batch [1067/1224], Loss: 0.014013719744980335\n",
      "Batch [1068/1224], Loss: 0.012764444574713707\n",
      "Batch [1069/1224], Loss: 0.013816328719258308\n",
      "Batch [1070/1224], Loss: 0.01105235330760479\n",
      "Batch [1071/1224], Loss: 0.013970596715807915\n",
      "Batch [1072/1224], Loss: 0.01220809668302536\n",
      "Batch [1073/1224], Loss: 0.01153873186558485\n",
      "Batch [1074/1224], Loss: 0.014711996540427208\n",
      "Batch [1075/1224], Loss: 0.013801402412354946\n",
      "Batch [1076/1224], Loss: 0.014324282296001911\n",
      "Batch [1077/1224], Loss: 0.013078519143164158\n",
      "Batch [1078/1224], Loss: 0.01232954766601324\n",
      "Batch [1079/1224], Loss: 0.016160784289240837\n",
      "Batch [1080/1224], Loss: 0.015530544333159924\n",
      "Batch [1081/1224], Loss: 0.014088836498558521\n",
      "Batch [1082/1224], Loss: 0.014332015067338943\n",
      "Batch [1083/1224], Loss: 0.012196552939713001\n",
      "Batch [1084/1224], Loss: 0.01296873576939106\n",
      "Batch [1085/1224], Loss: 0.013685301877558231\n",
      "Batch [1086/1224], Loss: 0.015369575470685959\n",
      "Batch [1087/1224], Loss: 0.016137227416038513\n",
      "Batch [1088/1224], Loss: 0.011024191975593567\n",
      "Batch [1089/1224], Loss: 0.015597437508404255\n",
      "Batch [1090/1224], Loss: 0.0133086908608675\n",
      "Batch [1091/1224], Loss: 0.011729823425412178\n",
      "Batch [1092/1224], Loss: 0.015659399330615997\n",
      "Batch [1093/1224], Loss: 0.012806365266442299\n",
      "Batch [1094/1224], Loss: 0.014655784703791142\n",
      "Batch [1095/1224], Loss: 0.013678108341991901\n",
      "Batch [1096/1224], Loss: 0.016620542854070663\n",
      "Batch [1097/1224], Loss: 0.015972472727298737\n",
      "Batch [1098/1224], Loss: 0.013004804030060768\n",
      "Batch [1099/1224], Loss: 0.013409794308245182\n",
      "Batch [1100/1224], Loss: 0.01251772977411747\n",
      "Batch [1101/1224], Loss: 0.015575745142996311\n",
      "Batch [1102/1224], Loss: 0.013586826622486115\n",
      "Batch [1103/1224], Loss: 0.013752836734056473\n",
      "Batch [1104/1224], Loss: 0.014126290567219257\n",
      "Batch [1105/1224], Loss: 0.014450755901634693\n",
      "Batch [1106/1224], Loss: 0.013525356538593769\n",
      "Batch [1107/1224], Loss: 0.012939415872097015\n",
      "Batch [1108/1224], Loss: 0.012369594536721706\n",
      "Batch [1109/1224], Loss: 0.0137345464900136\n",
      "Batch [1110/1224], Loss: 0.013011225499212742\n",
      "Batch [1111/1224], Loss: 0.015559700317680836\n",
      "Batch [1112/1224], Loss: 0.013598690740764141\n",
      "Batch [1113/1224], Loss: 0.015729108825325966\n",
      "Batch [1114/1224], Loss: 0.014364100061357021\n",
      "Batch [1115/1224], Loss: 0.016067590564489365\n",
      "Batch [1116/1224], Loss: 0.01311433594673872\n",
      "Batch [1117/1224], Loss: 0.013330210000276566\n",
      "Batch [1118/1224], Loss: 0.015818703919649124\n",
      "Batch [1119/1224], Loss: 0.016159091144800186\n",
      "Batch [1120/1224], Loss: 0.015276618301868439\n",
      "Batch [1121/1224], Loss: 0.014812619425356388\n",
      "Batch [1122/1224], Loss: 0.01356470212340355\n",
      "Batch [1123/1224], Loss: 0.013233636505901814\n",
      "Batch [1124/1224], Loss: 0.013396549038589\n",
      "Batch [1125/1224], Loss: 0.012961132451891899\n",
      "Batch [1126/1224], Loss: 0.017004966735839844\n",
      "Batch [1127/1224], Loss: 0.014012007974088192\n",
      "Batch [1128/1224], Loss: 0.016543906182050705\n",
      "Batch [1129/1224], Loss: 0.01379051897674799\n",
      "Batch [1130/1224], Loss: 0.015778610482811928\n",
      "Batch [1131/1224], Loss: 0.01467037107795477\n",
      "Batch [1132/1224], Loss: 0.012386780232191086\n",
      "Batch [1133/1224], Loss: 0.013477077707648277\n",
      "Batch [1134/1224], Loss: 0.012623026967048645\n",
      "Batch [1135/1224], Loss: 0.013836586847901344\n",
      "Batch [1136/1224], Loss: 0.0107274129986763\n",
      "Batch [1137/1224], Loss: 0.01416886318475008\n",
      "Batch [1138/1224], Loss: 0.015629222616553307\n",
      "Batch [1139/1224], Loss: 0.014011028222739697\n",
      "Batch [1140/1224], Loss: 0.015126550570130348\n",
      "Batch [1141/1224], Loss: 0.012949767522513866\n",
      "Batch [1142/1224], Loss: 0.012863602489233017\n",
      "Batch [1143/1224], Loss: 0.014190532267093658\n",
      "Batch [1144/1224], Loss: 0.012451623566448689\n",
      "Batch [1145/1224], Loss: 0.012053289450705051\n",
      "Batch [1146/1224], Loss: 0.015511754900217056\n",
      "Batch [1147/1224], Loss: 0.013120303861796856\n",
      "Batch [1148/1224], Loss: 0.011669013649225235\n",
      "Batch [1149/1224], Loss: 0.012918074615299702\n",
      "Batch [1150/1224], Loss: 0.018900210037827492\n",
      "Batch [1151/1224], Loss: 0.01346595585346222\n",
      "Batch [1152/1224], Loss: 0.013239274732768536\n",
      "Batch [1153/1224], Loss: 0.012786487117409706\n",
      "Batch [1154/1224], Loss: 0.014288915321230888\n",
      "Batch [1155/1224], Loss: 0.013230555690824986\n",
      "Batch [1156/1224], Loss: 0.013450318947434425\n",
      "Batch [1157/1224], Loss: 0.01220786664634943\n",
      "Batch [1158/1224], Loss: 0.014486750587821007\n",
      "Batch [1159/1224], Loss: 0.012863964773714542\n",
      "Batch [1160/1224], Loss: 0.01422782801091671\n",
      "Batch [1161/1224], Loss: 0.01553200464695692\n",
      "Batch [1162/1224], Loss: 0.014184567146003246\n",
      "Batch [1163/1224], Loss: 0.014851594343781471\n",
      "Batch [1164/1224], Loss: 0.014279714785516262\n",
      "Batch [1165/1224], Loss: 0.013771113939583302\n",
      "Batch [1166/1224], Loss: 0.01292418409138918\n",
      "Batch [1167/1224], Loss: 0.014048952609300613\n",
      "Batch [1168/1224], Loss: 0.012975599616765976\n",
      "Batch [1169/1224], Loss: 0.014482599683105946\n",
      "Batch [1170/1224], Loss: 0.010395871475338936\n",
      "Batch [1171/1224], Loss: 0.012064019218087196\n",
      "Batch [1172/1224], Loss: 0.014374695718288422\n",
      "Batch [1173/1224], Loss: 0.014013491570949554\n",
      "Batch [1174/1224], Loss: 0.015586892142891884\n",
      "Batch [1175/1224], Loss: 0.012537922710180283\n",
      "Batch [1176/1224], Loss: 0.014838626608252525\n",
      "Batch [1177/1224], Loss: 0.013560065068304539\n",
      "Batch [1178/1224], Loss: 0.013838092796504498\n",
      "Batch [1179/1224], Loss: 0.012582157738506794\n",
      "Batch [1180/1224], Loss: 0.011548962444067001\n",
      "Batch [1181/1224], Loss: 0.010919698514044285\n",
      "Batch [1182/1224], Loss: 0.013662837445735931\n",
      "Batch [1183/1224], Loss: 0.01592370867729187\n",
      "Batch [1184/1224], Loss: 0.013417472131550312\n",
      "Batch [1185/1224], Loss: 0.013994251377880573\n",
      "Batch [1186/1224], Loss: 0.012407148256897926\n",
      "Batch [1187/1224], Loss: 0.012716138735413551\n",
      "Batch [1188/1224], Loss: 0.014049471355974674\n",
      "Batch [1189/1224], Loss: 0.014507236890494823\n",
      "Batch [1190/1224], Loss: 0.011296779848635197\n",
      "Batch [1191/1224], Loss: 0.015836186707019806\n",
      "Batch [1192/1224], Loss: 0.014825689606368542\n",
      "Batch [1193/1224], Loss: 0.015773313120007515\n",
      "Batch [1194/1224], Loss: 0.012788970023393631\n",
      "Batch [1195/1224], Loss: 0.014760389924049377\n",
      "Batch [1196/1224], Loss: 0.015857184305787086\n",
      "Batch [1197/1224], Loss: 0.01384169701486826\n",
      "Batch [1198/1224], Loss: 0.014057982712984085\n",
      "Batch [1199/1224], Loss: 0.01709812693297863\n",
      "Batch [1200/1224], Loss: 0.013702793046832085\n",
      "Batch [1201/1224], Loss: 0.014987403526902199\n",
      "Batch [1202/1224], Loss: 0.010614403523504734\n",
      "Batch [1203/1224], Loss: 0.012669319286942482\n",
      "Batch [1204/1224], Loss: 0.015657730400562286\n",
      "Batch [1205/1224], Loss: 0.012879646383225918\n",
      "Batch [1206/1224], Loss: 0.015967603772878647\n",
      "Batch [1207/1224], Loss: 0.012325259856879711\n",
      "Batch [1208/1224], Loss: 0.013563214801251888\n",
      "Batch [1209/1224], Loss: 0.017702646553516388\n",
      "Batch [1210/1224], Loss: 0.013109436258673668\n",
      "Batch [1211/1224], Loss: 0.013364414684474468\n",
      "Batch [1212/1224], Loss: 0.01441548578441143\n",
      "Batch [1213/1224], Loss: 0.013465749099850655\n",
      "Batch [1214/1224], Loss: 0.012741204351186752\n",
      "Batch [1215/1224], Loss: 0.014910115860402584\n",
      "Batch [1216/1224], Loss: 0.013802656903862953\n",
      "Batch [1217/1224], Loss: 0.015028588473796844\n",
      "Batch [1218/1224], Loss: 0.013599992729723454\n",
      "Batch [1219/1224], Loss: 0.014279247261583805\n",
      "Batch [1220/1224], Loss: 0.01378567609935999\n",
      "Batch [1221/1224], Loss: 0.012986193411052227\n",
      "Batch [1222/1224], Loss: 0.011392972432076931\n",
      "Batch [1223/1224], Loss: 0.013570001348853111\n",
      "Batch [1224/1224], Loss: 0.013611259870231152\n",
      "Epoch [1/1], Loss: 0.01434247205536082\n",
      "Validation Batch [1/263], Loss: 0.014005784876644611\n",
      "Validation Batch [2/263], Loss: 0.012788865715265274\n",
      "Validation Batch [3/263], Loss: 0.012715011835098267\n",
      "Validation Batch [4/263], Loss: 0.015553632751107216\n",
      "Validation Batch [5/263], Loss: 0.013877028599381447\n",
      "Validation Batch [6/263], Loss: 0.017159482464194298\n",
      "Validation Batch [7/263], Loss: 0.01631111651659012\n",
      "Validation Batch [8/263], Loss: 0.01409507542848587\n",
      "Validation Batch [9/263], Loss: 0.017088353633880615\n",
      "Validation Batch [10/263], Loss: 0.012787014245986938\n",
      "Validation Batch [11/263], Loss: 0.015698468312621117\n",
      "Validation Batch [12/263], Loss: 0.01575389876961708\n",
      "Validation Batch [13/263], Loss: 0.013024849817156792\n",
      "Validation Batch [14/263], Loss: 0.01142861694097519\n",
      "Validation Batch [15/263], Loss: 0.012806151993572712\n",
      "Validation Batch [16/263], Loss: 0.01347795594483614\n",
      "Validation Batch [17/263], Loss: 0.014221355319023132\n",
      "Validation Batch [18/263], Loss: 0.011981706134974957\n",
      "Validation Batch [19/263], Loss: 0.015442642383277416\n",
      "Validation Batch [20/263], Loss: 0.013688070699572563\n",
      "Validation Batch [21/263], Loss: 0.014626136049628258\n",
      "Validation Batch [22/263], Loss: 0.015471430495381355\n",
      "Validation Batch [23/263], Loss: 0.015510150231420994\n",
      "Validation Batch [24/263], Loss: 0.01320152822881937\n",
      "Validation Batch [25/263], Loss: 0.010276315733790398\n",
      "Validation Batch [26/263], Loss: 0.01574832759797573\n",
      "Validation Batch [27/263], Loss: 0.012296542525291443\n",
      "Validation Batch [28/263], Loss: 0.011884674429893494\n",
      "Validation Batch [29/263], Loss: 0.014861095696687698\n",
      "Validation Batch [30/263], Loss: 0.015032432973384857\n",
      "Validation Batch [31/263], Loss: 0.010886694304645061\n",
      "Validation Batch [32/263], Loss: 0.018410734832286835\n",
      "Validation Batch [33/263], Loss: 0.015385687351226807\n",
      "Validation Batch [34/263], Loss: 0.010185468941926956\n",
      "Validation Batch [35/263], Loss: 0.017477108165621758\n",
      "Validation Batch [36/263], Loss: 0.012743020430207253\n",
      "Validation Batch [37/263], Loss: 0.013566117733716965\n",
      "Validation Batch [38/263], Loss: 0.013761068694293499\n",
      "Validation Batch [39/263], Loss: 0.012757486663758755\n",
      "Validation Batch [40/263], Loss: 0.012609749101102352\n",
      "Validation Batch [41/263], Loss: 0.013369783759117126\n",
      "Validation Batch [42/263], Loss: 0.012591859325766563\n",
      "Validation Batch [43/263], Loss: 0.011271480470895767\n",
      "Validation Batch [44/263], Loss: 0.013622998259961605\n",
      "Validation Batch [45/263], Loss: 0.011738872155547142\n",
      "Validation Batch [46/263], Loss: 0.011356746777892113\n",
      "Validation Batch [47/263], Loss: 0.017261283472180367\n",
      "Validation Batch [48/263], Loss: 0.01599365472793579\n",
      "Validation Batch [49/263], Loss: 0.013630520552396774\n",
      "Validation Batch [50/263], Loss: 0.014676200225949287\n",
      "Validation Batch [51/263], Loss: 0.013505630195140839\n",
      "Validation Batch [52/263], Loss: 0.013144271448254585\n",
      "Validation Batch [53/263], Loss: 0.01187975239008665\n",
      "Validation Batch [54/263], Loss: 0.012141703628003597\n",
      "Validation Batch [55/263], Loss: 0.015790708363056183\n",
      "Validation Batch [56/263], Loss: 0.014293942600488663\n",
      "Validation Batch [57/263], Loss: 0.013230802491307259\n",
      "Validation Batch [58/263], Loss: 0.015751853585243225\n",
      "Validation Batch [59/263], Loss: 0.012714974582195282\n",
      "Validation Batch [60/263], Loss: 0.014380188658833504\n",
      "Validation Batch [61/263], Loss: 0.01332521066069603\n",
      "Validation Batch [62/263], Loss: 0.014481551013886929\n",
      "Validation Batch [63/263], Loss: 0.014162009581923485\n",
      "Validation Batch [64/263], Loss: 0.01632372848689556\n",
      "Validation Batch [65/263], Loss: 0.011156954802572727\n",
      "Validation Batch [66/263], Loss: 0.012339966371655464\n",
      "Validation Batch [67/263], Loss: 0.014467275701463223\n",
      "Validation Batch [68/263], Loss: 0.008966739289462566\n",
      "Validation Batch [69/263], Loss: 0.016628289595246315\n",
      "Validation Batch [70/263], Loss: 0.013491102494299412\n",
      "Validation Batch [71/263], Loss: 0.013833860866725445\n",
      "Validation Batch [72/263], Loss: 0.012914144434034824\n",
      "Validation Batch [73/263], Loss: 0.015517947264015675\n",
      "Validation Batch [74/263], Loss: 0.014410304836928844\n",
      "Validation Batch [75/263], Loss: 0.014149321243166924\n",
      "Validation Batch [76/263], Loss: 0.013189522549510002\n",
      "Validation Batch [77/263], Loss: 0.013457006774842739\n",
      "Validation Batch [78/263], Loss: 0.01281425915658474\n",
      "Validation Batch [79/263], Loss: 0.014041372574865818\n",
      "Validation Batch [80/263], Loss: 0.014586133882403374\n",
      "Validation Batch [81/263], Loss: 0.013692784123122692\n",
      "Validation Batch [82/263], Loss: 0.012001748196780682\n",
      "Validation Batch [83/263], Loss: 0.013799531385302544\n",
      "Validation Batch [84/263], Loss: 0.015681486576795578\n",
      "Validation Batch [85/263], Loss: 0.013587603345513344\n",
      "Validation Batch [86/263], Loss: 0.013811899349093437\n",
      "Validation Batch [87/263], Loss: 0.014196418225765228\n",
      "Validation Batch [88/263], Loss: 0.01474288571625948\n",
      "Validation Batch [89/263], Loss: 0.013771808706223965\n",
      "Validation Batch [90/263], Loss: 0.01203170046210289\n",
      "Validation Batch [91/263], Loss: 0.015519275330007076\n",
      "Validation Batch [92/263], Loss: 0.018291763961315155\n",
      "Validation Batch [93/263], Loss: 0.014614135958254337\n",
      "Validation Batch [94/263], Loss: 0.011235782876610756\n",
      "Validation Batch [95/263], Loss: 0.014292553998529911\n",
      "Validation Batch [96/263], Loss: 0.01316758245229721\n",
      "Validation Batch [97/263], Loss: 0.013489114120602608\n",
      "Validation Batch [98/263], Loss: 0.012780367396771908\n",
      "Validation Batch [99/263], Loss: 0.014837739989161491\n",
      "Validation Batch [100/263], Loss: 0.017890295013785362\n",
      "Validation Batch [101/263], Loss: 0.011576496064662933\n",
      "Validation Batch [102/263], Loss: 0.013662864454090595\n",
      "Validation Batch [103/263], Loss: 0.010305576026439667\n",
      "Validation Batch [104/263], Loss: 0.01447015255689621\n",
      "Validation Batch [105/263], Loss: 0.013361657969653606\n",
      "Validation Batch [106/263], Loss: 0.012720847502350807\n",
      "Validation Batch [107/263], Loss: 0.011813140474259853\n",
      "Validation Batch [108/263], Loss: 0.012889684177935123\n",
      "Validation Batch [109/263], Loss: 0.015485897660255432\n",
      "Validation Batch [110/263], Loss: 0.016828196123242378\n",
      "Validation Batch [111/263], Loss: 0.015405387617647648\n",
      "Validation Batch [112/263], Loss: 0.01476617343723774\n",
      "Validation Batch [113/263], Loss: 0.014679884538054466\n",
      "Validation Batch [114/263], Loss: 0.012886540964245796\n",
      "Validation Batch [115/263], Loss: 0.01398694422096014\n",
      "Validation Batch [116/263], Loss: 0.011799242347478867\n",
      "Validation Batch [117/263], Loss: 0.013585836626589298\n",
      "Validation Batch [118/263], Loss: 0.014588788151741028\n",
      "Validation Batch [119/263], Loss: 0.013733535073697567\n",
      "Validation Batch [120/263], Loss: 0.010576115921139717\n",
      "Validation Batch [121/263], Loss: 0.011798652820289135\n",
      "Validation Batch [122/263], Loss: 0.016181645914912224\n",
      "Validation Batch [123/263], Loss: 0.014603661373257637\n",
      "Validation Batch [124/263], Loss: 0.012696481309831142\n",
      "Validation Batch [125/263], Loss: 0.013057942502200603\n",
      "Validation Batch [126/263], Loss: 0.018795348703861237\n",
      "Validation Batch [127/263], Loss: 0.014921018853783607\n",
      "Validation Batch [128/263], Loss: 0.01681118831038475\n",
      "Validation Batch [129/263], Loss: 0.013390212319791317\n",
      "Validation Batch [130/263], Loss: 0.015472253784537315\n",
      "Validation Batch [131/263], Loss: 0.018704069778323174\n",
      "Validation Batch [132/263], Loss: 0.013269597664475441\n",
      "Validation Batch [133/263], Loss: 0.012924439273774624\n",
      "Validation Batch [134/263], Loss: 0.01546813640743494\n",
      "Validation Batch [135/263], Loss: 0.013367822393774986\n",
      "Validation Batch [136/263], Loss: 0.014167730696499348\n",
      "Validation Batch [137/263], Loss: 0.012947438284754753\n",
      "Validation Batch [138/263], Loss: 0.014168852008879185\n",
      "Validation Batch [139/263], Loss: 0.015756580978631973\n",
      "Validation Batch [140/263], Loss: 0.012454742565751076\n",
      "Validation Batch [141/263], Loss: 0.01322377473115921\n",
      "Validation Batch [142/263], Loss: 0.017808446660637856\n",
      "Validation Batch [143/263], Loss: 0.013251949101686478\n",
      "Validation Batch [144/263], Loss: 0.013916601426899433\n",
      "Validation Batch [145/263], Loss: 0.013383792713284492\n",
      "Validation Batch [146/263], Loss: 0.015387700870633125\n",
      "Validation Batch [147/263], Loss: 0.012760632671415806\n",
      "Validation Batch [148/263], Loss: 0.011995287612080574\n",
      "Validation Batch [149/263], Loss: 0.015069407410919666\n",
      "Validation Batch [150/263], Loss: 0.012965939939022064\n",
      "Validation Batch [151/263], Loss: 0.015609808266162872\n",
      "Validation Batch [152/263], Loss: 0.015363328158855438\n",
      "Validation Batch [153/263], Loss: 0.013768432661890984\n",
      "Validation Batch [154/263], Loss: 0.015730157494544983\n",
      "Validation Batch [155/263], Loss: 0.010214400477707386\n",
      "Validation Batch [156/263], Loss: 0.012582997791469097\n",
      "Validation Batch [157/263], Loss: 0.014557484537363052\n",
      "Validation Batch [158/263], Loss: 0.013309195637702942\n",
      "Validation Batch [159/263], Loss: 0.015603257343173027\n",
      "Validation Batch [160/263], Loss: 0.0169325340539217\n",
      "Validation Batch [161/263], Loss: 0.01165633462369442\n",
      "Validation Batch [162/263], Loss: 0.013278808444738388\n",
      "Validation Batch [163/263], Loss: 0.014041741378605366\n",
      "Validation Batch [164/263], Loss: 0.014920104295015335\n",
      "Validation Batch [165/263], Loss: 0.01342722401022911\n",
      "Validation Batch [166/263], Loss: 0.012042286805808544\n",
      "Validation Batch [167/263], Loss: 0.014107053168118\n",
      "Validation Batch [168/263], Loss: 0.017166418954730034\n",
      "Validation Batch [169/263], Loss: 0.015227350406348705\n",
      "Validation Batch [170/263], Loss: 0.015795234590768814\n",
      "Validation Batch [171/263], Loss: 0.01269588340073824\n",
      "Validation Batch [172/263], Loss: 0.014370773918926716\n",
      "Validation Batch [173/263], Loss: 0.01495442260056734\n",
      "Validation Batch [174/263], Loss: 0.011544980108737946\n",
      "Validation Batch [175/263], Loss: 0.014662371948361397\n",
      "Validation Batch [176/263], Loss: 0.015778468921780586\n",
      "Validation Batch [177/263], Loss: 0.013317257165908813\n",
      "Validation Batch [178/263], Loss: 0.015302088111639023\n",
      "Validation Batch [179/263], Loss: 0.015200510621070862\n",
      "Validation Batch [180/263], Loss: 0.013615638017654419\n",
      "Validation Batch [181/263], Loss: 0.015589085407555103\n",
      "Validation Batch [182/263], Loss: 0.013632654212415218\n",
      "Validation Batch [183/263], Loss: 0.016841888427734375\n",
      "Validation Batch [184/263], Loss: 0.0130830192938447\n",
      "Validation Batch [185/263], Loss: 0.013146154582500458\n",
      "Validation Batch [186/263], Loss: 0.01293885800987482\n",
      "Validation Batch [187/263], Loss: 0.015720974653959274\n",
      "Validation Batch [188/263], Loss: 0.014514514245092869\n",
      "Validation Batch [189/263], Loss: 0.012217353098094463\n",
      "Validation Batch [190/263], Loss: 0.012273889034986496\n",
      "Validation Batch [191/263], Loss: 0.0157962404191494\n",
      "Validation Batch [192/263], Loss: 0.01450329925864935\n",
      "Validation Batch [193/263], Loss: 0.011635443195700645\n",
      "Validation Batch [194/263], Loss: 0.016962366178631783\n",
      "Validation Batch [195/263], Loss: 0.014771579764783382\n",
      "Validation Batch [196/263], Loss: 0.01390223577618599\n",
      "Validation Batch [197/263], Loss: 0.01488460786640644\n",
      "Validation Batch [198/263], Loss: 0.014302393421530724\n",
      "Validation Batch [199/263], Loss: 0.01419705618172884\n",
      "Validation Batch [200/263], Loss: 0.014795875176787376\n",
      "Validation Batch [201/263], Loss: 0.013472414575517178\n",
      "Validation Batch [202/263], Loss: 0.012422793544828892\n",
      "Validation Batch [203/263], Loss: 0.013002431951463223\n",
      "Validation Batch [204/263], Loss: 0.016161270439624786\n",
      "Validation Batch [205/263], Loss: 0.017948072403669357\n",
      "Validation Batch [206/263], Loss: 0.014674131758511066\n",
      "Validation Batch [207/263], Loss: 0.012763794511556625\n",
      "Validation Batch [208/263], Loss: 0.01219986006617546\n",
      "Validation Batch [209/263], Loss: 0.015338110737502575\n",
      "Validation Batch [210/263], Loss: 0.013656700029969215\n",
      "Validation Batch [211/263], Loss: 0.011859181337058544\n",
      "Validation Batch [212/263], Loss: 0.012160259298980236\n",
      "Validation Batch [213/263], Loss: 0.012201981619000435\n",
      "Validation Batch [214/263], Loss: 0.013414173386991024\n",
      "Validation Batch [215/263], Loss: 0.014487070962786674\n",
      "Validation Batch [216/263], Loss: 0.014008184894919395\n",
      "Validation Batch [217/263], Loss: 0.012809031642973423\n",
      "Validation Batch [218/263], Loss: 0.014246485196053982\n",
      "Validation Batch [219/263], Loss: 0.013319237157702446\n",
      "Validation Batch [220/263], Loss: 0.01421090867370367\n",
      "Validation Batch [221/263], Loss: 0.016352467238903046\n",
      "Validation Batch [222/263], Loss: 0.014186156913638115\n",
      "Validation Batch [223/263], Loss: 0.0098684411495924\n",
      "Validation Batch [224/263], Loss: 0.016272922977805138\n",
      "Validation Batch [225/263], Loss: 0.011873960494995117\n",
      "Validation Batch [226/263], Loss: 0.016492903232574463\n",
      "Validation Batch [227/263], Loss: 0.012980904430150986\n",
      "Validation Batch [228/263], Loss: 0.013905541971325874\n",
      "Validation Batch [229/263], Loss: 0.01233082078397274\n",
      "Validation Batch [230/263], Loss: 0.012377806939184666\n",
      "Validation Batch [231/263], Loss: 0.015196425840258598\n",
      "Validation Batch [232/263], Loss: 0.012724201194941998\n",
      "Validation Batch [233/263], Loss: 0.01630380190908909\n",
      "Validation Batch [234/263], Loss: 0.014907490462064743\n",
      "Validation Batch [235/263], Loss: 0.017441054806113243\n",
      "Validation Batch [236/263], Loss: 0.01313849352300167\n",
      "Validation Batch [237/263], Loss: 0.013640843331813812\n",
      "Validation Batch [238/263], Loss: 0.013798444531857967\n",
      "Validation Batch [239/263], Loss: 0.013073029927909374\n",
      "Validation Batch [240/263], Loss: 0.011299707926809788\n",
      "Validation Batch [241/263], Loss: 0.01410480123013258\n",
      "Validation Batch [242/263], Loss: 0.014824544079601765\n",
      "Validation Batch [243/263], Loss: 0.015826011076569557\n",
      "Validation Batch [244/263], Loss: 0.014567391015589237\n",
      "Validation Batch [245/263], Loss: 0.013178447261452675\n",
      "Validation Batch [246/263], Loss: 0.018180882558226585\n",
      "Validation Batch [247/263], Loss: 0.014905822463333607\n",
      "Validation Batch [248/263], Loss: 0.013453824445605278\n",
      "Validation Batch [249/263], Loss: 0.013863964006304741\n",
      "Validation Batch [250/263], Loss: 0.014145378023386002\n",
      "Validation Batch [251/263], Loss: 0.015536310151219368\n",
      "Validation Batch [252/263], Loss: 0.013313776813447475\n",
      "Validation Batch [253/263], Loss: 0.011407220736145973\n",
      "Validation Batch [254/263], Loss: 0.0109636215493083\n",
      "Validation Batch [255/263], Loss: 0.013215989805758\n",
      "Validation Batch [256/263], Loss: 0.013801724649965763\n",
      "Validation Batch [257/263], Loss: 0.015093800611793995\n",
      "Validation Batch [258/263], Loss: 0.015335675328969955\n",
      "Validation Batch [259/263], Loss: 0.015960151329636574\n",
      "Validation Batch [260/263], Loss: 0.014014576561748981\n",
      "Validation Batch [261/263], Loss: 0.013432852923870087\n",
      "Validation Batch [262/263], Loss: 0.013865804299712181\n",
      "Validation Batch [263/263], Loss: 0.014804631471633911\n",
      "Validation Loss: 0.01400489095924245\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        print(f'Batch [{i+1}/{len(train_loader)}], Loss: {loss.item()}')\n",
    "        i += 1\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "            print(f'Validation Batch [{i+1}/{len(val_loader)}], Loss: {loss.item()}')\n",
    "            i += 1\n",
    "\n",
    "    print(f'Validation Loss: {val_running_loss/len(val_loader)}')\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), 'xray_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 28, in <module>\n",
      "    from .rnn import RNNBase, RNN, LSTM, GRU, \\\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/nn/modules/rnn.py\", line 11, in <module>\n",
      "    from ..utils.rnn import PackedSequence\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/nn/utils/__init__.py\", line 8, in <module>\n",
      "    from . import parametrizations\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/nn/utils/parametrizations.py\", line 5, in <module>\n",
      "    from ..utils import parametrize\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1091, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1191, in get_data\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torchvision/__init__.py\", line 6, in <module>\n",
      "    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torchvision/models/__init__.py\", line 2, in <module>\n",
      "    from .convnext import *\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torchvision/models/convnext.py\", line 8, in <module>\n",
      "    from ..ops.misc import Conv2dNormActivation, Permute\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torchvision/ops/__init__.py\", line 23, in <module>\n",
      "    from .poolers import MultiScaleRoIAlign\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torchvision/ops/poolers.py\", line 10, in <module>\n",
      "    from .roi_align import roi_align\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torchvision/ops/roi_align.py\", line 4, in <module>\n",
      "    import torch._dynamo\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/_dynamo/__init__.py\", line 2, in <module>\n",
      "    from . import allowed_functions, convert_frame, eval_frame, resume_execution\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 62, in <module>\n",
      "    from .output_graph import OutputGraph\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 89, in <module>\n",
      "    from .variables.builder import GraphArg, TrackedFake, VariableBuilder, wrap_fx_proxy\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/_dynamo/variables/builder.py\", line 143, in <module>\n",
      "    from .optimizer import OptimizerVariable\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/_dynamo/variables/optimizer.py\", line 5, in <module>\n",
      "    from ..decorators import mark_static_address\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/_dynamo/decorators.py\", line 140, in <module>\n",
      "    @_disallow_in_graph_helper(throw_if_not_allowed=False)\n",
      "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/_dynamo/decorators.py\", line 109, in inner\n",
      "    allowed_functions._allowed_function_ids.remove(id(fn))\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/_dynamo/allowed_functions.py\", line 88, in remove\n",
      "    function_ids = self()\n",
      "                   ^^^^^^\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/_dynamo/allowed_functions.py\", line 69, in __call__\n",
      "    value = self.lazy_initializer()\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/_dynamo/allowed_functions.py\", line 417, in _allowed_function_ids\n",
      "    return gen_allowed_objs_and_ids().object_ids\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/_dynamo/allowed_functions.py\", line 375, in gen_allowed_objs_and_ids\n",
      "    _find_torch_objects(torch)\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/_dynamo/allowed_functions.py\", line 363, in _find_torch_objects\n",
      "    _find_torch_objects(obj)\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/_dynamo/allowed_functions.py\", line 364, in _find_torch_objects\n",
      "    elif _is_allowed_module_prefix(obj):\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/_dynamo/allowed_functions.py\", line 322, in _is_allowed_module_prefix\n",
      "    if any(mod_name.startswith(m) for m in disallowed_modules):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ruchireddy/ne_ds4440/.conda/lib/python3.12/site-packages/torch/_dynamo/allowed_functions.py\", line 322, in <genexpr>\n",
      "    if any(mod_name.startswith(m) for m in disallowed_modules):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m all_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ne_ds4440/.conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ne_ds4440/.conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ne_ds4440/.conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/ne_ds4440/.conda/lib/python3.12/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/ne_ds4440/.conda/lib/python3.12/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ne_ds4440/.conda/lib/python3.12/multiprocessing/context.py:289\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ne_ds4440/.conda/lib/python3.12/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ne_ds4440/.conda/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ne_ds4440/.conda/lib/python3.12/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load('xray_model.pth', map_location=device))\n",
    "\n",
    "# Validation loop\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        predictions = (probabilities > 0.5).float()\n",
    "\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "\n",
    "# Calculate classification report\n",
    "report = classification_report(all_labels, all_predictions, target_names=LABELS, output_dict=True)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Print F1 scores, precision, and recall for each class\n",
    "for class_name, metrics in report.items():\n",
    "    if class_name not in ['accuracy', 'macro avg']:\n",
    "        print(f\"Class: {class_name}\")\n",
    "        print(f\"  Precision: {metrics['precision']}\")\n",
    "        print(f\"  Recall: {metrics['recall']}\")\n",
    "        print(f\"  F1 Score: {metrics['f1-score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def create_tensorboard_visualization(model_path, data_loader, device):\n",
    "    # Load the model\n",
    "    model = XRayModel(num_classes=len(LABELS))\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Create a SummaryWriter\n",
    "    writer = SummaryWriter(log_dir='logs/tensorboard')\n",
    "\n",
    "    # Extract embeddings and labels\n",
    "    embeddings = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for images, labels in data_loader:\n",
    "            print('iter:', i)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get the embeddings from the layer before the output layer\n",
    "            features = model.densenet.features(images)\n",
    "            embeddings.append(features.cpu())\n",
    "            labels_list.append(labels.cpu())\n",
    "            i += 1\n",
    "\n",
    "    # Concatenate all embeddings and labels\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    labels_list = torch.cat(labels_list, dim=0)\n",
    "\n",
    "    # Add embeddings to TensorBoard\n",
    "    writer.add_embedding(embeddings, metadata=labels_list, label_img=images.cpu())\n",
    "\n",
    "    # Close the writer\n",
    "    writer.close()\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have a DataLoader named test_loader\n",
    "# create_tensorboard_visualization(\"xray_model.pth\", test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.18.0 at http://localhost:6007/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=logs/tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m#  Ensure `device` is correctly formatted\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mcreate_tensorboard_visualization\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxray_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 49\u001b[0m, in \u001b[0;36mcreate_tensorboard_visualization\u001b[0;34m(model_path, test_loader, device)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_tensorboard_visualization\u001b[39m(model_path, test_loader, device):\n\u001b[1;32m     48\u001b[0m     model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m     50\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")  #  Ensure `device` is correctly formatted\n",
    "\n",
    "create_tensorboard_visualization(\"xray_model.pth\", test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
